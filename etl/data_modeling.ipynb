{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# complex math\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# data preparation\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# data modeling\n",
    "\n",
    "\n",
    "# data scoring\n",
    "\n",
    "\n",
    "# data tuning   \n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# pickle and .env\n",
    "from dotenv import dotenv_values\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_round(x, pos): \n",
    "    if abs(x) >= 1e9: \n",
    "        return f'{x/1e9} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x}'\n",
    "    \n",
    "def val_round(x):\n",
    "    if abs(x) >= 1e9:\n",
    "        return f'{x/1e9:.2f} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6:.2f} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3:.2f} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk konversi tipe data\n",
    "def convert_object_columns_to_numeric(df):\n",
    "    for col in df.select_dtypes(include = ['object']).columns:  \n",
    "        try:\n",
    "            # Cek apakah semua nilai bisa dikonversi ke float\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "            \n",
    "            # Jika bisa, ubah ke int jika semua nilai adalah bilangan bulat\n",
    "            if all(df[col] % 1 == 0):  # Cek apakah semua nilai adalah bilangan bulat\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Jika ada nilai non-angka, biarkan tetap object\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 283712 entries, 0 to 283711\n",
      "Data columns (total 25 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   credit_card                283712 non-null  int64         \n",
      " 1   datetime                   283712 non-null  datetime64[ns]\n",
      " 2   long                       283712 non-null  float64       \n",
      " 3   lat                        283712 non-null  float64       \n",
      " 4   zipcode                    283712 non-null  int64         \n",
      " 5   state                      283712 non-null  object        \n",
      " 6   city                       283712 non-null  object        \n",
      " 7   year                       283712 non-null  int32         \n",
      " 8   quarter                    283712 non-null  object        \n",
      " 9   month                      283712 non-null  object        \n",
      " 10  season                     283712 non-null  object        \n",
      " 11  week_cat                   283712 non-null  object        \n",
      " 12  day                        283712 non-null  object        \n",
      " 13  credit_card_limit          283712 non-null  int64         \n",
      " 14  limit_cat                  283712 non-null  object        \n",
      " 15  transaction_dollar_amount  283712 non-null  float64       \n",
      " 16  transaction_count          283712 non-null  float64       \n",
      " 17  time_diff_per_seconds      283712 non-null  float64       \n",
      " 18  prev_long                  283712 non-null  float64       \n",
      " 19  prev_lat                   283712 non-null  float64       \n",
      " 20  distance                   283712 non-null  float64       \n",
      " 21  geo_cat                    283712 non-null  object        \n",
      " 22  fraud_status               283712 non-null  object        \n",
      " 23  cc_id                      283712 non-null  object        \n",
      " 24  trx_id                     283712 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(8), int32(1), int64(3), object(12)\n",
      "memory usage: 53.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "share = {**dotenv_values('../.env.shared')} \n",
    "\n",
    "# read pickle\n",
    "with open(share['CLEAN_DATA'], 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "cc_df = pd.DataFrame(loaded_data)\n",
    "cc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "credit_card",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "long",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "zipcode",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "quarter",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "month",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "season",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "week_cat",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "credit_card_limit",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "limit_cat",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "transaction_dollar_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "transaction_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_diff_per_seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "prev_long",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "prev_lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "geo_cat",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fraud_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cc_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "trx_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "468e9041-574a-45e9-a5e1-6e843e967a17",
       "rows": [
        [
         "0",
         "9484591448272784",
         "2015-07-31 09:39:48",
         "-90.0456390041606",
         "29.8890387448395",
         "70112",
         "la",
         "new orleans",
         "2015",
         "2015Q3",
         "july",
         "summer",
         "weekday",
         "friday",
         "4000",
         "very_low",
         "17.99",
         "1.0",
         "-7642455.0",
         "-90.1515044715485",
         "29.9452019714035",
         "11.969568192523129",
         "normal",
         "not_fraud",
         "07be7585e04b3f7e4a77b836ed48ec92f3097af384f2443fc37c5847dec50b62",
         "f116a89c9d151ceae51bfac4f5621638780a630231e956343427f700383cad4e"
        ],
        [
         "1",
         "7053196367895112",
         "2015-07-31 11:03:48",
         "-74.0275614326073",
         "40.6896150543123",
         "10001",
         "ny",
         "new york",
         "2015",
         "2015Q3",
         "july",
         "summer",
         "weekday",
         "friday",
         "18000",
         "low",
         "12.09",
         "1.0",
         "-2527299.0",
         "-73.9270292384803",
         "40.8065107929275",
         "15.511210253249892",
         "normal",
         "not_fraud",
         "c80431b67c993ae6be172bbe1b4d674aec83be187642a589a444f2027ec6dc72",
         "ee75c3f8ab7bca52cf783239208b402826593542361ed0d04792605aa62ffce9"
        ],
        [
         "2",
         "9528285469413252",
         "2015-07-31 11:10:14",
         "-72.1394845669351",
         "43.1081002008752",
         "3280",
         "nh",
         "washington",
         "2015",
         "2015Q3",
         "july",
         "summer",
         "weekday",
         "friday",
         "40000",
         "very_high",
         "78.21",
         "1.0",
         "-6508550.0",
         "-72.064112755835",
         "43.1722807441918",
         "9.404225777907621",
         "normal",
         "not_fraud",
         "d4c74fccfec999aea34abb4716639d77a6f1d2acb445e31deffd43ff33ad3553",
         "2cba57e3b7b18d280c89ee5a023599c5562093fce5402fdd928b537d22f42f03"
        ],
        [
         "3",
         "1845720274833905",
         "2015-07-31 11:28:55",
         "-89.0021481284464",
         "40.8043234778234",
         "61738",
         "il",
         "el paso",
         "2015",
         "2015Q3",
         "july",
         "summer",
         "weekday",
         "friday",
         "20000",
         "medium",
         "74.41",
         "1.0",
         "-2534699.0",
         "-88.974491675175",
         "40.720876673461",
         "9.556418502988254",
         "normal",
         "not_fraud",
         "9d066f44cb7867c8a3f90e55ed1ca26ce0bda402edb638aebffa1df6eb33f13b",
         "416801c2eb00d305a9508a8ab613bb22726a86cc25c7812f3cf0dce07e4bbce1"
        ],
        [
         "4",
         "7850942767136368",
         "2015-07-31 11:38:51",
         "-72.0256746243903",
         "43.2107528971734",
         "3280",
         "nh",
         "washington",
         "2015",
         "2015Q3",
         "july",
         "summer",
         "weekday",
         "friday",
         "4000",
         "very_low",
         "54.89",
         "1.0",
         "-1785659.0",
         "-72.1253922707213",
         "43.2192233835488",
         "8.157130205866816",
         "normal",
         "not_fraud",
         "53c396633b079d36a554e0b360a62e9a8d3518650e233e8e7edbeee2f20f2999",
         "028d8551b67034944945071fba350331b8264f09cb233bd2a92dbd4250132873"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_card</th>\n",
       "      <th>datetime</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>week_cat</th>\n",
       "      <th>day</th>\n",
       "      <th>credit_card_limit</th>\n",
       "      <th>limit_cat</th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>prev_long</th>\n",
       "      <th>prev_lat</th>\n",
       "      <th>distance</th>\n",
       "      <th>geo_cat</th>\n",
       "      <th>fraud_status</th>\n",
       "      <th>cc_id</th>\n",
       "      <th>trx_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9484591448272784</td>\n",
       "      <td>2015-07-31 09:39:48</td>\n",
       "      <td>-90.045639</td>\n",
       "      <td>29.889039</td>\n",
       "      <td>70112</td>\n",
       "      <td>la</td>\n",
       "      <td>new orleans</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>4000</td>\n",
       "      <td>very_low</td>\n",
       "      <td>17.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7642455.0</td>\n",
       "      <td>-90.151504</td>\n",
       "      <td>29.945202</td>\n",
       "      <td>11.969568</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>07be7585e04b3f7e4a77b836ed48ec92f3097af384f244...</td>\n",
       "      <td>f116a89c9d151ceae51bfac4f5621638780a630231e956...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7053196367895112</td>\n",
       "      <td>2015-07-31 11:03:48</td>\n",
       "      <td>-74.027561</td>\n",
       "      <td>40.689615</td>\n",
       "      <td>10001</td>\n",
       "      <td>ny</td>\n",
       "      <td>new york</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>18000</td>\n",
       "      <td>low</td>\n",
       "      <td>12.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2527299.0</td>\n",
       "      <td>-73.927029</td>\n",
       "      <td>40.806511</td>\n",
       "      <td>15.511210</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>c80431b67c993ae6be172bbe1b4d674aec83be187642a5...</td>\n",
       "      <td>ee75c3f8ab7bca52cf783239208b402826593542361ed0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9528285469413252</td>\n",
       "      <td>2015-07-31 11:10:14</td>\n",
       "      <td>-72.139485</td>\n",
       "      <td>43.108100</td>\n",
       "      <td>3280</td>\n",
       "      <td>nh</td>\n",
       "      <td>washington</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>40000</td>\n",
       "      <td>very_high</td>\n",
       "      <td>78.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6508550.0</td>\n",
       "      <td>-72.064113</td>\n",
       "      <td>43.172281</td>\n",
       "      <td>9.404226</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>d4c74fccfec999aea34abb4716639d77a6f1d2acb445e3...</td>\n",
       "      <td>2cba57e3b7b18d280c89ee5a023599c5562093fce5402f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1845720274833905</td>\n",
       "      <td>2015-07-31 11:28:55</td>\n",
       "      <td>-89.002148</td>\n",
       "      <td>40.804323</td>\n",
       "      <td>61738</td>\n",
       "      <td>il</td>\n",
       "      <td>el paso</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>20000</td>\n",
       "      <td>medium</td>\n",
       "      <td>74.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2534699.0</td>\n",
       "      <td>-88.974492</td>\n",
       "      <td>40.720877</td>\n",
       "      <td>9.556419</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>9d066f44cb7867c8a3f90e55ed1ca26ce0bda402edb638...</td>\n",
       "      <td>416801c2eb00d305a9508a8ab613bb22726a86cc25c781...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7850942767136368</td>\n",
       "      <td>2015-07-31 11:38:51</td>\n",
       "      <td>-72.025675</td>\n",
       "      <td>43.210753</td>\n",
       "      <td>3280</td>\n",
       "      <td>nh</td>\n",
       "      <td>washington</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>4000</td>\n",
       "      <td>very_low</td>\n",
       "      <td>54.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1785659.0</td>\n",
       "      <td>-72.125392</td>\n",
       "      <td>43.219223</td>\n",
       "      <td>8.157130</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>53c396633b079d36a554e0b360a62e9a8d3518650e233e...</td>\n",
       "      <td>028d8551b67034944945071fba350331b8264f09cb233b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        credit_card            datetime       long        lat  zipcode state  \\\n",
       "0  9484591448272784 2015-07-31 09:39:48 -90.045639  29.889039    70112    la   \n",
       "1  7053196367895112 2015-07-31 11:03:48 -74.027561  40.689615    10001    ny   \n",
       "2  9528285469413252 2015-07-31 11:10:14 -72.139485  43.108100     3280    nh   \n",
       "3  1845720274833905 2015-07-31 11:28:55 -89.002148  40.804323    61738    il   \n",
       "4  7850942767136368 2015-07-31 11:38:51 -72.025675  43.210753     3280    nh   \n",
       "\n",
       "          city  year quarter month  season week_cat     day  \\\n",
       "0  new orleans  2015  2015Q3  july  summer  weekday  friday   \n",
       "1     new york  2015  2015Q3  july  summer  weekday  friday   \n",
       "2   washington  2015  2015Q3  july  summer  weekday  friday   \n",
       "3      el paso  2015  2015Q3  july  summer  weekday  friday   \n",
       "4   washington  2015  2015Q3  july  summer  weekday  friday   \n",
       "\n",
       "   credit_card_limit  limit_cat  transaction_dollar_amount  transaction_count  \\\n",
       "0               4000   very_low                      17.99                1.0   \n",
       "1              18000        low                      12.09                1.0   \n",
       "2              40000  very_high                      78.21                1.0   \n",
       "3              20000     medium                      74.41                1.0   \n",
       "4               4000   very_low                      54.89                1.0   \n",
       "\n",
       "   time_diff_per_seconds  prev_long   prev_lat   distance geo_cat  \\\n",
       "0             -7642455.0 -90.151504  29.945202  11.969568  normal   \n",
       "1             -2527299.0 -73.927029  40.806511  15.511210  normal   \n",
       "2             -6508550.0 -72.064113  43.172281   9.404226  normal   \n",
       "3             -2534699.0 -88.974492  40.720877   9.556419  normal   \n",
       "4             -1785659.0 -72.125392  43.219223   8.157130  normal   \n",
       "\n",
       "  fraud_status                                              cc_id  \\\n",
       "0    not_fraud  07be7585e04b3f7e4a77b836ed48ec92f3097af384f244...   \n",
       "1    not_fraud  c80431b67c993ae6be172bbe1b4d674aec83be187642a5...   \n",
       "2    not_fraud  d4c74fccfec999aea34abb4716639d77a6f1d2acb445e3...   \n",
       "3    not_fraud  9d066f44cb7867c8a3f90e55ed1ca26ce0bda402edb638...   \n",
       "4    not_fraud  53c396633b079d36a554e0b360a62e9a8d3518650e233e...   \n",
       "\n",
       "                                              trx_id  \n",
       "0  f116a89c9d151ceae51bfac4f5621638780a630231e956...  \n",
       "1  ee75c3f8ab7bca52cf783239208b402826593542361ed0...  \n",
       "2  2cba57e3b7b18d280c89ee5a023599c5562093fce5402f...  \n",
       "3  416801c2eb00d305a9508a8ab613bb22726a86cc25c781...  \n",
       "4  028d8551b67034944945071fba350331b8264f09cb233b...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and Irrelevant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Threshold Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric columns: Index(['credit_card', 'long', 'lat', 'zipcode', 'year', 'credit_card_limit',\n",
      "       'transaction_dollar_amount', 'transaction_count',\n",
      "       'time_diff_per_seconds', 'prev_long', 'prev_lat', 'distance'],\n",
      "      dtype='object')\n",
      "\n",
      "Fitur yang dipertahankan: Index(['credit_card', 'long', 'lat', 'zipcode', 'credit_card_limit',\n",
      "       'transaction_dollar_amount', 'time_diff_per_seconds', 'prev_long',\n",
      "       'prev_lat', 'distance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Drop kolom non-numerik\n",
    "df_numeric = cc_df.select_dtypes(include = ['number'])\n",
    "print(f'numeric columns: {df_numeric.columns}\\n')\n",
    "\n",
    "# Inisialisasi VarianceThreshold (misalnya, ambang batas 0.01)\n",
    "selector = VarianceThreshold(threshold = 0.01)\n",
    "df_var_selected = selector.fit_transform(df_numeric)\n",
    "\n",
    "# Fitur yang dipertahankan\n",
    "selected_features = df_numeric.columns[selector.get_support()]\n",
    "print(\"Fitur yang dipertahankan:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric column untuk modeling: Index(['transaction_dollar_amount', 'time_diff_per_seconds', 'distance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Seleceted numeric columns\n",
    "filter_numeric = ['credit_card', 'long', 'lat', 'zipcode', 'credit_card_limit', 'prev_long', 'prev_lat']\n",
    "selected_numeric = selected_features.drop(filter_numeric)\n",
    "\n",
    "#\n",
    "print(\"Numeric column untuk modeling:\", selected_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Relevant Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE \t: ['la' 'ny' 'nh' 'il' 'pa' 'nj' 'mo' 'md' 'ca' 'tx' 'me' 'vt' 'al' 'wv'\n",
      " 'pr' 'wa' 'nc' 'ga' 'ma' 'ok' 'mi' 'ut' 'fl' 'hi' 'ia' 'nm' 'oh' 'az'\n",
      " 'va' 'in' 'ri' 'id' 'co' 'ct' 'ks'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "CITY \t: ['new orleans' 'new york' 'washington' 'el paso' 'dallas' 'houston'\n",
      " 'birmingham' 'kansas city' 'austin' 'pasadena' 'los angeles' 'fort worth'\n",
      " 'jackson' 'pittsburgh' 'portland' 'albany' 'charlotte' 'huntsville'\n",
      " 'madison' 'orlando' 'san antonio' 'seattle' 'minneapolis' 'sacramento'\n",
      " 'san francisco' 'memphis' 'dayton' 'denver' 'milwaukee' 'omaha' 'trenton'\n",
      " 'springfield' 'oklahoma city' 'charleston' 'miami' 'long beach' 'quitman'\n",
      " 'saint louis' 'friendship' 'chicago' 'salt lake city' 'richmond'\n",
      " 'pensacola' 'san diego' 'atlanta' 'honolulu' 'greensboro' 'newark'\n",
      " 'rochester' 'lafayette' 'columbus' 'staten island' 'des moines'\n",
      " 'las vegas' 'chester' 'cincinnati' 'hillsboro' 'tucson' 'buffalo'\n",
      " 'arlington' 'shreveport' 'philadelphia' 'tulsa' 'cleveland' 'saint paul'\n",
      " 'young america' 'clinton' 'amarillo' 'greenville' 'mobile' 'boise'\n",
      " 'monticello' 'indianapolis' 'cascade' 'williamsburg' 'raleigh' 'akron'\n",
      " 'huntington' 'troy' 'lake city' 'colorado springs' 'phoenix' 'fresno'\n",
      " 'auburn' 'garfield' 'evansville' 'topeka' 'cedar rapids' 'louisville'\n",
      " 'knoxville' 'oakland' 'spokane' 'manchester' 'fort wayne' 'dover' 'tampa'\n",
      " 'garden grove' 'lexington' 'alexandria' 'tacoma' 'jamaica' 'scranton'\n",
      " 'hartford' 'columbia' 'gretna' 'san jose' 'aurora' 'jacksonville'\n",
      " 'somerset' 'new haven' 'newport' 'wilmington' 'boston' 'vallejo'\n",
      " 'fort lauderdale' 'bronx' 'wichita' 'lancaster' 'detroit' 'baltimore'\n",
      " 'bristol' 'corpus christi' 'roanoke'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "QUARTER \t: ['2015Q3' '2015Q4'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "MONTH \t: ['july' 'august' 'september' 'october'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "SEASON \t: ['summer' 'fall'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "WEEK_CAT \t: ['weekday' 'weekend'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "DAY \t: ['friday' 'saturday' 'sunday' 'monday' 'tuesday' 'wednesday' 'thursday'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "LIMIT_CAT \t: ['very_low' 'low' 'very_high' 'medium' 'high'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "GEO_CAT \t: ['normal' 'anomaly'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "FRAUD_STATUS \t: ['not_fraud' 'fraud'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "CC_ID \t: ['07be7585e04b3f7e4a77b836ed48ec92f3097af384f2443fc37c5847dec50b62'\n",
      " 'c80431b67c993ae6be172bbe1b4d674aec83be187642a589a444f2027ec6dc72'\n",
      " 'd4c74fccfec999aea34abb4716639d77a6f1d2acb445e31deffd43ff33ad3553' ...\n",
      " '758a4abfca673e48e32ff0b29dce558a868323865bb3fb7ddd397b266044c075'\n",
      " 'c15b62f22db812830cd158d392c86d7bcc905b829078909f50dfe67cb9113a5f'\n",
      " 'd585b64a134c46d1afa5dcc8e644042dbae051004f48581ecbd7f012957e931c'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "TRX_ID \t: ['f116a89c9d151ceae51bfac4f5621638780a630231e956343427f700383cad4e'\n",
      " 'ee75c3f8ab7bca52cf783239208b402826593542361ed0d04792605aa62ffce9'\n",
      " '2cba57e3b7b18d280c89ee5a023599c5562093fce5402fdd928b537d22f42f03' ...\n",
      " '06e28924688c7a97191512c2fffc88ad4274ec997d477bf3583bfcc5a86a3127'\n",
      " '3cf844389ecb1b621c4372cdc20b3c3f2e495d6455f71cc38fe71e0370f29f0a'\n",
      " 'fcbeca269b90fd922bf3dfed2ffac395482f184dc7d5831ebf9b833fad47282a'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Column Category\n",
    "check_cat = cc_df.select_dtypes(include = ['object'])\n",
    "\n",
    "for i in check_cat.columns:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()} \\n')\n",
    "    print(f'{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objetc columns: Index(['state', 'city', 'quarter', 'month', 'season', 'week_cat', 'day',\n",
      "       'limit_cat', 'geo_cat', 'fraud_status', 'cc_id', 'trx_id'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop kolom numerik\n",
    "df_obj = cc_df.select_dtypes(include = ['object'])\n",
    "print(f'objetc columns: {df_obj.columns}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object column untuk modeling: Index(['limit_cat', 'fraud_status', 'geo_cat'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# selected object columns\n",
    "filter_obj = ['limit_cat', 'fraud_status', 'geo_cat']\n",
    "selected_object = df_obj[filter_obj].columns\n",
    "\n",
    "#\n",
    "print(\"Object column untuk modeling:\", selected_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "transaction_dollar_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "time_diff_per_seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "limit_cat",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fraud_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "geo_cat",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "38356f61-dcba-4292-a129-2bd668c7a9ac",
       "rows": [
        [
         "0",
         "17.99",
         "-7642455.0",
         "11.969568192523129",
         "very_low",
         "not_fraud",
         "normal"
        ],
        [
         "1",
         "12.09",
         "-2527299.0",
         "15.511210253249892",
         "low",
         "not_fraud",
         "normal"
        ],
        [
         "2",
         "78.21",
         "-6508550.0",
         "9.404225777907621",
         "very_high",
         "not_fraud",
         "normal"
        ],
        [
         "3",
         "74.41",
         "-2534699.0",
         "9.556418502988254",
         "medium",
         "not_fraud",
         "normal"
        ],
        [
         "4",
         "54.89",
         "-1785659.0",
         "8.157130205866816",
         "very_low",
         "not_fraud",
         "normal"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>distance</th>\n",
       "      <th>limit_cat</th>\n",
       "      <th>fraud_status</th>\n",
       "      <th>geo_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>-7642455.0</td>\n",
       "      <td>11.969568</td>\n",
       "      <td>very_low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.09</td>\n",
       "      <td>-2527299.0</td>\n",
       "      <td>15.511210</td>\n",
       "      <td>low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.21</td>\n",
       "      <td>-6508550.0</td>\n",
       "      <td>9.404226</td>\n",
       "      <td>very_high</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.41</td>\n",
       "      <td>-2534699.0</td>\n",
       "      <td>9.556419</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.89</td>\n",
       "      <td>-1785659.0</td>\n",
       "      <td>8.157130</td>\n",
       "      <td>very_low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_dollar_amount  time_diff_per_seconds   distance  limit_cat  \\\n",
       "0                      17.99             -7642455.0  11.969568   very_low   \n",
       "1                      12.09             -2527299.0  15.511210        low   \n",
       "2                      78.21             -6508550.0   9.404226  very_high   \n",
       "3                      74.41             -2534699.0   9.556419     medium   \n",
       "4                      54.89             -1785659.0   8.157130   very_low   \n",
       "\n",
       "  fraud_status geo_cat  \n",
       "0    not_fraud  normal  \n",
       "1    not_fraud  normal  \n",
       "2    not_fraud  normal  \n",
       "3    not_fraud  normal  \n",
       "4    not_fraud  normal  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "model_col = selected_numeric.append(selected_object)\n",
    "\n",
    "# \n",
    "model_col = cc_df[model_col]\n",
    "model_col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transaction_dollar_amount', 'time_diff_per_seconds', 'distance',\n",
       "       'limit_cat', 'fraud_status', 'geo_cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove NaN from target\n",
    "model_df = model_col.dropna(subset = ['fraud_status'])\n",
    "model_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud_status\n",
      "not_fraud    98.25\n",
      "fraud         1.75\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(round(model_df[\"fraud_status\"].value_counts(normalize = True) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "X = model_df.drop(columns = [\"fraud_status\"]).copy()\n",
    "y = model_df[\"fraud_status\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Encoding Columns: ['limit_cat']\n",
      "One-Hot Encoding Columns: ['geo_cat']\n",
      "Numeric Columns: ['transaction_dollar_amount', 'time_diff_per_seconds', 'distance']\n"
     ]
    }
   ],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "encoding_set = {'limit_cat'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "ordinal_cols = []\n",
    "one_hot_cols = []\n",
    "numeric_cols = []\n",
    "\n",
    "# Mengelompokkan kolom berdasarkan tipe data\n",
    "for col in model_df.columns:\n",
    "    if col == \"fraud_status\":  # Pastikan kolom target tidak masuk sebagai fitur\n",
    "        continue\n",
    "    \n",
    "    if model_df[col].dtype in ['int64', 'float64']:  \n",
    "        numeric_cols.append(col)\n",
    "\n",
    "    elif model_df[col].dtype == 'object' or model_df[col].dtype.name == \"category\":\n",
    "        if col in encoding_set:\n",
    "            ordinal_cols.append(col)\n",
    "            \n",
    "        else:\n",
    "            one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Columns:\", ordinal_cols)\n",
    "print(\"One-Hot Columns:\", one_hot_cols)\n",
    "print(\"Numeric Columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_CAT \t: ['very_low' 'low' 'very_high' 'medium' 'high']\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Ordinal Columns\n",
    "for i in ordinal_cols:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()}')\n",
    "    print(f'{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menentukan urutan kategori masing-masing kolom\n",
    "oridnal_cat = [\n",
    "    [\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"],   # Urutan untuk limit_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan semua kolom yang akan diproses ada dalam X_train\n",
    "missing_cols = set(numeric_cols + one_hot_cols + ordinal_cols) - set(X_train.columns)\n",
    "assert not missing_cols, f\"Kolom berikut hilang dari X_train: {missing_cols}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan SMOTE dengan parameter khusus\n",
    "smote = SMOTE(\n",
    "    sampling_strategy = 0.8,  # Minoritas menjadi 80% dari mayoritas\n",
    "    k_neighbors = NearestNeighbors(n_jobs = -1),  # Multi-threading dengan NearestNeighbors\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# Definisikan TomekLinks dengan parameter khusus\n",
    "tomek = TomekLinks(\n",
    "    sampling_strategy = 'majority'  # Hanya menghapus data mayoritas yang memiliki pasangan Tomek\n",
    ")\n",
    "\n",
    "sampling = SMOTETomek(\n",
    "        smote = smote,  # Gunakan SMOTE yang dikustomisasi\n",
    "        tomek = tomek,  # Gunakan TomekLinks yang dikustomisasi\n",
    "        random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore', sparse_output = True, max_categories = 50)\n",
    "ordinal_transformer = OrdinalEncoder(categories = oridnal_cat, handle_unknown = 'use_encoded_value', unknown_value = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Column Transformer\n",
    "prep_stage_2 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_transformer, numeric_cols), \n",
    "        (\"cat\", categorical_transformer, one_hot_cols), \n",
    "        (\"ord\", ordinal_transformer, ordinal_cols)\n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangani kasus ZeroDivisionError jika kelas minoritas tidak ada di y_train\n",
    "if np.sum(y_train == 1) == 0:\n",
    "    scale_pos_weight = 1\n",
    "else:\n",
    "    scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline untuk model dengan preprocessing dan SMOTETomek yang dikustomisasi\n",
    "rfc_pipe = Pipeline([\n",
    "    (\"preprocessor\", prep_stage_2),  # Langkah preprocessing\n",
    "    (\"sampling\", sampling),\n",
    "    (\"rfc\", RandomForestClassifier(\n",
    "        n_estimators = 200,  # Lebih banyak pohon untuk performa lebih baik\n",
    "        max_depth = 10,  # Batasi kedalaman untuk menghindari overfitting\n",
    "        random_state = 42,\n",
    "        class_weight = \"balanced\",\n",
    "        n_jobs = -1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg_pipe = Pipeline([\n",
    "    (\"preprocessor\", prep_stage_2),\n",
    "    (\"sampling\", sampling),\n",
    "    (\"model\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe = Pipeline([\n",
    "    (\"preprocessor\", prep_stage_2),\n",
    "    (\"sampling\", sampling),\n",
    "    (\"model\", XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light BGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ï¸âƒ£ LightGBM\n",
    "lgbm_pipe = Pipeline([\n",
    "    (\"preprocessor\", prep_stage_2),\n",
    "    (\"sampling\", sampling),\n",
    "    (\"model\", LGBMClassifier(is_unbalance=True, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_pipe = Pipeline([\n",
    "    (\"preprocessor\", prep_stage_2),\n",
    "    (\"sampling\", sampling),\n",
    "    (\"model\", CatBoostClassifier(auto_class_weights='Balanced', verbose=0, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Evaluasi Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       fraud       1.00      1.00      1.00       995\n",
      "   not_fraud       1.00      1.00      1.00     55748\n",
      "\n",
      "    accuracy                           1.00     56743\n",
      "   macro avg       1.00      1.00      1.00     56743\n",
      "weighted avg       1.00      1.00      1.00     56743\n",
      "\n",
      "ROC-AUC Score: 1.0000\n",
      "\n",
      "ðŸ”¹ Evaluasi Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       fraud       1.00      1.00      1.00       995\n",
      "   not_fraud       1.00      1.00      1.00     55748\n",
      "\n",
      "    accuracy                           1.00     56743\n",
      "   macro avg       1.00      1.00      1.00     56743\n",
      "weighted avg       1.00      1.00      1.00     56743\n",
      "\n",
      "ROC-AUC Score: 1.0000\n",
      "\n",
      "ðŸ”¹ Evaluasi Model: XGBoost\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1], got ['fraud' 'not_fraud']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, pipe \u001b[38;5;129;01min\u001b[39;00m pipelines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ”¹ Evaluasi Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     17\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Prasetya\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Prasetya\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py:526\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    521\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    522\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    523\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    524\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    525\u001b[0m         )\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, yt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Prasetya\\anaconda3\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Prasetya\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got ['fraud' 'not_fraud']"
     ]
    }
   ],
   "source": [
    "# Menyimpan semua pipeline dalam dictionary\n",
    "pipelines = {\n",
    "    \"Random Forest\": rfc_pipe, \n",
    "    \"Logistic Regression\": logreg_pipe,\n",
    "    \"XGBoost\": xgb_pipe,\n",
    "    \"LightGBM\": lgbm_pipe,\n",
    "    \"CatBoost\": catboost_pipe\n",
    "}\n",
    "\n",
    "# Evaluasi semua model\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"\\nðŸ”¹ Evaluasi Model: {name}\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_pred_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Train model pertama kali di X_train**\n",
    "rfc_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Ambil preprocessor dari pipeline\n",
    "preprocessor = rfc_pipe.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Transformasi data menggunakan preprocessor\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Ambil nama kolom numerik\n",
    "numeric_feature_names = numeric_cols\n",
    "\n",
    "# Ambil nama kolom One-Hot Encoding\n",
    "one_hot_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(one_hot_cols)\n",
    "\n",
    "# Gabungkan semua nama kolom\n",
    "all_feature_names = (list(numeric_feature_names) + \n",
    "                     list(one_hot_feature_names) + \n",
    "                     list(ordinal_cols))\n",
    "\n",
    "# Buat DataFrame dengan nama kolom asli\n",
    "processed_df = pd.DataFrame(\n",
    "    X_train_transformed.toarray() if hasattr(X_train_transformed, \"toarray\") else X_train_transformed,\n",
    "    columns=all_feature_names\n",
    ")\n",
    "\n",
    "# Menampilkan DataFrame hasil preprocessing\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan total null pada setiap kolom\n",
    "null_columns = processed_df.isnull().sum()[processed_df.isnull().sum() > 0]\n",
    "print(f'Total null columns: {null_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 evaluasi pada train dataset\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Prediksi pada data uji\n",
    "y_train_pred = rfc_pipe.predict(X_train)\n",
    "\n",
    "# Evaluasi dengan classification report\n",
    "classification_rep = classification_report(y_train, y_train_pred)\n",
    "print(f'Classification Report: {classification_rep}')\n",
    "\n",
    "# Prediksi probabilistik\n",
    "y__train_pred_proba = rfc_pipe.predict_proba(X_train)[:, 1]  # Probabilitas kelas positif (fraud)\n",
    "\n",
    "# Evaluasi dengan ROC-AUC\n",
    "roc_auc_rep = roc_auc_score(y_train, y__train_pred_proba)\n",
    "print(f'ROC-AUC Score: {roc_auc_rep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek distribusi kelas\n",
    "print(f'{y_train.value_counts(normalize = True)} \\n')  # Menampilkan proporsi kelas\n",
    "\n",
    "# \n",
    "unique, counts = np.unique(y_train_pred, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data leak from\n",
    "print(f'duplicate on x_train: {X_train.duplicated().sum()}')\n",
    "print(f'duplicate on y_train: {y_train.duplicated().sum()} \\n')\n",
    "\n",
    "# Cek apakah ada baris duplikat secara keseluruhan\n",
    "df_train = X_train.copy()\n",
    "df_train['target'] = y_train  # Gabungkan dengan target\n",
    "\n",
    "print(f'duplicate on train data: {df_train.duplicated().sum()}')  # Cek duplikasi di seluruh dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 evaluasi pada test dataset\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Prediksi pada data uji\n",
    "y_test_pred = rfc_pipe.predict(X_test)\n",
    "\n",
    "# Evaluasi dengan classification report\n",
    "classification_rep = classification_report(y_test, y_test_pred)\n",
    "print(f'Classification Report: {classification_rep}')\n",
    "\n",
    "# Prediksi probabilistik\n",
    "y__test_pred_proba = rfc_pipe.predict_proba(X_test)[:, 1]  # Probabilitas kelas positif (fraud)\n",
    "\n",
    "# Evaluasi dengan ROC-AUC\n",
    "roc_auc_rep = roc_auc_score(y_test, y__test_pred_proba)\n",
    "print(f'ROC-AUC Score: {roc_auc_rep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek distribusi kelas\n",
    "print(f'{y_test.value_counts(normalize = True)} \\n')  # Menampilkan proporsi kelas\n",
    "\n",
    "# \n",
    "unique, counts = np.unique(y_test_pred, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data leak from\n",
    "print(f'duplicate on x_train: {X_test.duplicated().sum()}')\n",
    "print(f'duplicate on y_train: {y_test.duplicated().sum()} \\n')\n",
    "\n",
    "# Cek apakah ada baris duplikat secara keseluruhan\n",
    "df_test = X_test.copy()\n",
    "df_test['target'] = y_test  # Gabungkan dengan target\n",
    "\n",
    "print(f'duplicate on test data: {df_test.duplicated().sum()}')  # Cek duplikasi di seluruh dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Optimal CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Daftar K yang ingin diuji\n",
    "k_values = [3, 5, 7, 10]  # Bisa diperluas jika perlu\n",
    "scores = []\n",
    "\n",
    "# Evaluasi dengan berbagai K-Fold\n",
    "for k in k_values:\n",
    "    cv = StratifiedKFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "    score = np.mean(cross_val_score(rfc_pipe, X_train, y_train, cv = cv, scoring = 'f1'))\n",
    "    scores.append((k, score))\n",
    "    print(f\"K = {k}: Mean Cross-Val Score = {score:.4f}\")\n",
    "\n",
    "print(f'{\"-\" * 50} \\n')\n",
    "\n",
    "# Menentukan nilai K terbaik\n",
    "best_k = sorted(scores, key = lambda x: x[1], reverse = True)[0][0]\n",
    "print(f\"Optimal K untuk Cross-Validation: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See Pattern CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded, _ = pd.factorize(y_train)  # Mengonversi label ke numerik\n",
    "\n",
    "for train_idx, val_idx in cv.split(X_train, y_train_encoded):  # Gunakan y_train_encoded\n",
    "    y_train_subset = y_train_encoded[train_idx]\n",
    "    y_val_subset = y_train_encoded[val_idx]\n",
    "\n",
    "    print(\"Train class distribution: \\t\", np.bincount(y_train_subset)) # [sample majority, sample minority]\n",
    "    print(\"Val class distribution: \\t\", np.bincount(y_val_subset))\n",
    "    print(f\"{'-' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Step 3: Train ulang model setelah cross-validation**\n",
    "# base_model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# # **Step 4: Cross-Validation pada Testing Set**\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # KFold dengan 5 lipatan\n",
    "# cv_scores_test = cross_val_score(base_model_pipe, X_test, y_test, cv = kf, scoring = 'recall')\n",
    "\n",
    "# # **Step 5: Evaluasi pada Testing Set**\n",
    "# print(\"Cross-validation accuracy on Testing Set:\", cv_scores_test)\n",
    "# print(\"Mean CV accuracy on Testing Set:\", cv_scores_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter & Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Fraud\", \"Fraud\"], yticklabels=[\"Non-Fraud\", \"Fraud\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# # Probabilitas prediksi\n",
    "# y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# # Hitung ROC Curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC Curve\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=\"AUC = {:.2f}\".format(roc_auc))\n",
    "# plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan folder tujuan\n",
    "dir_name = 'datamart'\n",
    "folder_path = f\"../{dir_name}\"\n",
    "\n",
    "# Cek apakah folder sudah ada, jika belum buat foldernya\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "    print(f\"Directory '{dir_name}' created successfully.\")\n",
    "\n",
    "else: \n",
    "    print(f'Directory has already been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # parameter\n",
    "# share = {**dotenv_values('../.env.shared')} \n",
    "\n",
    "# # Simpan model terbaik ke file\n",
    "# joblib.dump(best_model, share['FRAUD_DETECT'])\n",
    "\n",
    "# print(\"Model berhasil disimpan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model yang sudah disimpan\n",
    "\n",
    "# # parameter\n",
    "# share = {**dotenv_values('../.env.shared')} \n",
    "\n",
    "# loaded_model = joblib.load(share['FRAUD_DETECT'])\n",
    "\n",
    "# print(\"Model berhasil dimuat kembali!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediksi pada data baru\n",
    "# y_pred_new = loaded_model.predict(X_test)\n",
    "\n",
    "# # Evaluasi kembali model\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contoh data baru (pastikan sesuai format dataset)\n",
    "# new_transaction = np.array([[1000, 0, 1, 0, 500, 20]])  # Ubah sesuai dataset\n",
    "\n",
    "# # Standardisasi data baru jika sebelumnya menggunakan scaler\n",
    "# scaler = StandardScaler()\n",
    "# new_transaction_scaled = scaler.transform(new_transaction)\n",
    "\n",
    "# # Prediksi\n",
    "# prediction = loaded_model.predict(new_transaction_scaled)\n",
    "\n",
    "# # Hasil prediksi\n",
    "# print(\"Prediksi: Fraud\" if prediction[0] == 1 else \"Prediksi: Not Fraud\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
