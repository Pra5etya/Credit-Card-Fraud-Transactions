{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# graph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# complex math\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# data preparation\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# data modeling\n",
    "\n",
    "\n",
    "# data scoring\n",
    "\n",
    "\n",
    "# data tuning   \n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# pickle and .env\n",
    "from dotenv import dotenv_values\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_round(x, pos): \n",
    "    if abs(x) >= 1e9: \n",
    "        return f'{x/1e9} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x}'\n",
    "    \n",
    "def val_round(x):\n",
    "    if abs(x) >= 1e9:\n",
    "        return f'{x/1e9:.2f} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6:.2f} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3:.2f} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk konversi tipe data\n",
    "def convert_object_columns_to_numeric(df):\n",
    "    for col in df.select_dtypes(include = ['object']).columns:  \n",
    "        try:\n",
    "            # Cek apakah semua nilai bisa dikonversi ke float\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "            \n",
    "            # Jika bisa, ubah ke int jika semua nilai adalah bilangan bulat\n",
    "            if all(df[col] % 1 == 0):  # Cek apakah semua nilai adalah bilangan bulat\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Jika ada nilai non-angka, biarkan tetap object\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 283712 entries, 0 to 283711\n",
      "Data columns (total 25 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   credit_card                283712 non-null  int64         \n",
      " 1   datetime                   283712 non-null  datetime64[ns]\n",
      " 2   long                       283712 non-null  float64       \n",
      " 3   lat                        283712 non-null  float64       \n",
      " 4   zipcode                    283712 non-null  int64         \n",
      " 5   state                      283712 non-null  object        \n",
      " 6   city                       283712 non-null  object        \n",
      " 7   year                       283712 non-null  int32         \n",
      " 8   quarter                    283712 non-null  object        \n",
      " 9   month                      283712 non-null  object        \n",
      " 10  season                     283712 non-null  object        \n",
      " 11  week_cat                   283712 non-null  object        \n",
      " 12  day                        283712 non-null  object        \n",
      " 13  credit_card_limit          283712 non-null  int64         \n",
      " 14  limit_cat                  283712 non-null  object        \n",
      " 15  transaction_dollar_amount  283712 non-null  float64       \n",
      " 16  transaction_count          283712 non-null  float64       \n",
      " 17  time_diff_per_seconds      283712 non-null  float64       \n",
      " 18  prev_long                  283712 non-null  float64       \n",
      " 19  prev_lat                   283712 non-null  float64       \n",
      " 20  distance                   283712 non-null  float64       \n",
      " 21  geo_cat                    283712 non-null  object        \n",
      " 22  fraud_status               283712 non-null  object        \n",
      " 23  cc_id                      283712 non-null  object        \n",
      " 24  trx_id                     283712 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(8), int32(1), int64(3), object(12)\n",
      "memory usage: 53.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "share = {**dotenv_values('../.env.shared')} \n",
    "\n",
    "# read pickle\n",
    "with open(share['CLEAN_DATA'], 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "cc_df = pd.DataFrame(loaded_data)\n",
    "cc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_card</th>\n",
       "      <th>datetime</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>week_cat</th>\n",
       "      <th>day</th>\n",
       "      <th>credit_card_limit</th>\n",
       "      <th>limit_cat</th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>prev_long</th>\n",
       "      <th>prev_lat</th>\n",
       "      <th>distance</th>\n",
       "      <th>geo_cat</th>\n",
       "      <th>fraud_status</th>\n",
       "      <th>cc_id</th>\n",
       "      <th>trx_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9484591448272784</td>\n",
       "      <td>2015-07-31 09:39:48</td>\n",
       "      <td>-90.045639</td>\n",
       "      <td>29.889039</td>\n",
       "      <td>70112</td>\n",
       "      <td>la</td>\n",
       "      <td>new orleans</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>4000</td>\n",
       "      <td>very_low</td>\n",
       "      <td>17.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7642455.0</td>\n",
       "      <td>-90.151504</td>\n",
       "      <td>29.945202</td>\n",
       "      <td>11.969568</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>ac1e34e60d6ad33e82c597a0f269fe2b5e83428562d3aa...</td>\n",
       "      <td>0bc4a969dccbe3b475e9e374e53e9e3fce6dbf1e7da2fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7053196367895112</td>\n",
       "      <td>2015-07-31 11:03:48</td>\n",
       "      <td>-74.027561</td>\n",
       "      <td>40.689615</td>\n",
       "      <td>10001</td>\n",
       "      <td>ny</td>\n",
       "      <td>new york</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>18000</td>\n",
       "      <td>low</td>\n",
       "      <td>12.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2527299.0</td>\n",
       "      <td>-73.927029</td>\n",
       "      <td>40.806511</td>\n",
       "      <td>15.511210</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>1c266eb56e8271b57de874865469dc04abb5110ef52821...</td>\n",
       "      <td>03ba63876abb11634b3f875ddad559ee63940573628739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9528285469413252</td>\n",
       "      <td>2015-07-31 11:10:14</td>\n",
       "      <td>-72.139485</td>\n",
       "      <td>43.108100</td>\n",
       "      <td>3280</td>\n",
       "      <td>nh</td>\n",
       "      <td>washington</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>40000</td>\n",
       "      <td>very_high</td>\n",
       "      <td>78.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6508550.0</td>\n",
       "      <td>-72.064113</td>\n",
       "      <td>43.172281</td>\n",
       "      <td>9.404226</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>6733096fda61cddbcb8e2cd74676332d87594d058be167...</td>\n",
       "      <td>b86ab6aa560ba291acec2dd27b90f810165ff9023aab47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1845720274833905</td>\n",
       "      <td>2015-07-31 11:28:55</td>\n",
       "      <td>-89.002148</td>\n",
       "      <td>40.804323</td>\n",
       "      <td>61738</td>\n",
       "      <td>il</td>\n",
       "      <td>el paso</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>20000</td>\n",
       "      <td>medium</td>\n",
       "      <td>74.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2534699.0</td>\n",
       "      <td>-88.974492</td>\n",
       "      <td>40.720877</td>\n",
       "      <td>9.556419</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>c046d480aab2d35f98751ac74f030eff8d3c74005ac01c...</td>\n",
       "      <td>7e58fe9a9c6d89388acbd39be811095b6f13614fb16b93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7850942767136368</td>\n",
       "      <td>2015-07-31 11:38:51</td>\n",
       "      <td>-72.025675</td>\n",
       "      <td>43.210753</td>\n",
       "      <td>3280</td>\n",
       "      <td>nh</td>\n",
       "      <td>washington</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015Q3</td>\n",
       "      <td>july</td>\n",
       "      <td>summer</td>\n",
       "      <td>weekday</td>\n",
       "      <td>friday</td>\n",
       "      <td>4000</td>\n",
       "      <td>very_low</td>\n",
       "      <td>54.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1785659.0</td>\n",
       "      <td>-72.125392</td>\n",
       "      <td>43.219223</td>\n",
       "      <td>8.157130</td>\n",
       "      <td>normal</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>c59721adc2284ba7805c637ce4b1d25046d366d12833c0...</td>\n",
       "      <td>595746461886416a18a9ab75bde2742d402301a27b8f28...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        credit_card            datetime       long        lat  zipcode state  \\\n",
       "0  9484591448272784 2015-07-31 09:39:48 -90.045639  29.889039    70112    la   \n",
       "1  7053196367895112 2015-07-31 11:03:48 -74.027561  40.689615    10001    ny   \n",
       "2  9528285469413252 2015-07-31 11:10:14 -72.139485  43.108100     3280    nh   \n",
       "3  1845720274833905 2015-07-31 11:28:55 -89.002148  40.804323    61738    il   \n",
       "4  7850942767136368 2015-07-31 11:38:51 -72.025675  43.210753     3280    nh   \n",
       "\n",
       "          city  year quarter month  season week_cat     day  \\\n",
       "0  new orleans  2015  2015Q3  july  summer  weekday  friday   \n",
       "1     new york  2015  2015Q3  july  summer  weekday  friday   \n",
       "2   washington  2015  2015Q3  july  summer  weekday  friday   \n",
       "3      el paso  2015  2015Q3  july  summer  weekday  friday   \n",
       "4   washington  2015  2015Q3  july  summer  weekday  friday   \n",
       "\n",
       "   credit_card_limit  limit_cat  transaction_dollar_amount  transaction_count  \\\n",
       "0               4000   very_low                      17.99                1.0   \n",
       "1              18000        low                      12.09                1.0   \n",
       "2              40000  very_high                      78.21                1.0   \n",
       "3              20000     medium                      74.41                1.0   \n",
       "4               4000   very_low                      54.89                1.0   \n",
       "\n",
       "   time_diff_per_seconds  prev_long   prev_lat   distance geo_cat  \\\n",
       "0             -7642455.0 -90.151504  29.945202  11.969568  normal   \n",
       "1             -2527299.0 -73.927029  40.806511  15.511210  normal   \n",
       "2             -6508550.0 -72.064113  43.172281   9.404226  normal   \n",
       "3             -2534699.0 -88.974492  40.720877   9.556419  normal   \n",
       "4             -1785659.0 -72.125392  43.219223   8.157130  normal   \n",
       "\n",
       "  fraud_status                                              cc_id  \\\n",
       "0    not_fraud  ac1e34e60d6ad33e82c597a0f269fe2b5e83428562d3aa...   \n",
       "1    not_fraud  1c266eb56e8271b57de874865469dc04abb5110ef52821...   \n",
       "2    not_fraud  6733096fda61cddbcb8e2cd74676332d87594d058be167...   \n",
       "3    not_fraud  c046d480aab2d35f98751ac74f030eff8d3c74005ac01c...   \n",
       "4    not_fraud  c59721adc2284ba7805c637ce4b1d25046d366d12833c0...   \n",
       "\n",
       "                                              trx_id  \n",
       "0  0bc4a969dccbe3b475e9e374e53e9e3fce6dbf1e7da2fe...  \n",
       "1  03ba63876abb11634b3f875ddad559ee63940573628739...  \n",
       "2  b86ab6aa560ba291acec2dd27b90f810165ff9023aab47...  \n",
       "3  7e58fe9a9c6d89388acbd39be811095b6f13614fb16b93...  \n",
       "4  595746461886416a18a9ab75bde2742d402301a27b8f28...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and Irrelevant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Threshold Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric columns: Index(['credit_card', 'long', 'lat', 'zipcode', 'year', 'credit_card_limit',\n",
      "       'transaction_dollar_amount', 'transaction_count',\n",
      "       'time_diff_per_seconds', 'prev_long', 'prev_lat', 'distance'],\n",
      "      dtype='object')\n",
      "\n",
      "Fitur yang dipertahankan: Index(['credit_card', 'long', 'lat', 'zipcode', 'credit_card_limit',\n",
      "       'transaction_dollar_amount', 'time_diff_per_seconds', 'prev_long',\n",
      "       'prev_lat', 'distance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Drop kolom non-numerik\n",
    "df_numeric = cc_df.select_dtypes(include = ['number'])\n",
    "print(f'numeric columns: {df_numeric.columns}\\n')\n",
    "\n",
    "# Inisialisasi VarianceThreshold (misalnya, ambang batas 0.01)\n",
    "selector = VarianceThreshold(threshold = 0.01)\n",
    "df_var_selected = selector.fit_transform(df_numeric)\n",
    "\n",
    "# Fitur yang dipertahankan\n",
    "selected_features = df_numeric.columns[selector.get_support()]\n",
    "print(\"Fitur yang dipertahankan:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric column untuk modeling: Index(['transaction_dollar_amount', 'time_diff_per_seconds', 'distance'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Seleceted numeric columns\n",
    "filter_numeric = ['credit_card', 'long', 'lat', 'zipcode', 'credit_card_limit', 'prev_long', 'prev_lat']\n",
    "selected_numeric = selected_features.drop(filter_numeric)\n",
    "\n",
    "#\n",
    "print(\"Numeric column untuk modeling:\", selected_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Relevant Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE \t: ['la' 'ny' 'nh' 'il' 'pa' 'nj' 'mo' 'md' 'ca' 'tx' 'me' 'vt' 'al' 'wv'\n",
      " 'pr' 'wa' 'nc' 'ga' 'ma' 'ok' 'mi' 'ut' 'fl' 'hi' 'ia' 'nm' 'oh' 'az'\n",
      " 'va' 'in' 'ri' 'id' 'co' 'ct' 'ks'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "CITY \t: ['new orleans' 'new york' 'washington' 'el paso' 'dallas' 'houston'\n",
      " 'birmingham' 'kansas city' 'austin' 'pasadena' 'los angeles' 'fort worth'\n",
      " 'jackson' 'pittsburgh' 'portland' 'albany' 'charlotte' 'huntsville'\n",
      " 'madison' 'orlando' 'san antonio' 'seattle' 'minneapolis' 'sacramento'\n",
      " 'san francisco' 'memphis' 'dayton' 'denver' 'milwaukee' 'omaha' 'trenton'\n",
      " 'springfield' 'oklahoma city' 'charleston' 'miami' 'long beach' 'quitman'\n",
      " 'saint louis' 'friendship' 'chicago' 'salt lake city' 'richmond'\n",
      " 'pensacola' 'san diego' 'atlanta' 'honolulu' 'greensboro' 'newark'\n",
      " 'rochester' 'lafayette' 'columbus' 'staten island' 'des moines'\n",
      " 'las vegas' 'chester' 'cincinnati' 'hillsboro' 'tucson' 'buffalo'\n",
      " 'arlington' 'shreveport' 'philadelphia' 'tulsa' 'cleveland' 'saint paul'\n",
      " 'young america' 'clinton' 'amarillo' 'greenville' 'mobile' 'boise'\n",
      " 'monticello' 'indianapolis' 'cascade' 'williamsburg' 'raleigh' 'akron'\n",
      " 'huntington' 'troy' 'lake city' 'colorado springs' 'phoenix' 'fresno'\n",
      " 'auburn' 'garfield' 'evansville' 'topeka' 'cedar rapids' 'louisville'\n",
      " 'knoxville' 'oakland' 'spokane' 'manchester' 'fort wayne' 'dover' 'tampa'\n",
      " 'garden grove' 'lexington' 'alexandria' 'tacoma' 'jamaica' 'scranton'\n",
      " 'hartford' 'columbia' 'gretna' 'san jose' 'aurora' 'jacksonville'\n",
      " 'somerset' 'new haven' 'newport' 'wilmington' 'boston' 'vallejo'\n",
      " 'fort lauderdale' 'bronx' 'wichita' 'lancaster' 'detroit' 'baltimore'\n",
      " 'bristol' 'corpus christi' 'roanoke'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "QUARTER \t: ['2015Q3' '2015Q4'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "MONTH \t: ['july' 'august' 'september' 'october'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "SEASON \t: ['summer' 'fall'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "WEEK_CAT \t: ['weekday' 'weekend'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "DAY \t: ['friday' 'saturday' 'sunday' 'monday' 'tuesday' 'wednesday' 'thursday'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "LIMIT_CAT \t: ['very_low' 'low' 'very_high' 'medium' 'high'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "GEO_CAT \t: ['normal' 'anomaly'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "FRAUD_STATUS \t: ['not_fraud' 'fraud'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "CC_ID \t: ['ac1e34e60d6ad33e82c597a0f269fe2b5e83428562d3aabd566ee7bbd443f5d9'\n",
      " '1c266eb56e8271b57de874865469dc04abb5110ef52821756fc7cb5d40ba9b5e'\n",
      " '6733096fda61cddbcb8e2cd74676332d87594d058be16773a54427d6bb6f80ca' ...\n",
      " 'f75a5c688ac926a97d47828ad7c2309b8dd2dcb2879c6550e34c59db1cb673fa'\n",
      " 'e5a25692318622e2a82cb61b42af51abe2244e73dc0e33daf4a1650052eca98d'\n",
      " '09759c6a2afb74247dbbf11e78973f678f9351264e87125f96d9e2d8c843a4b9'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "TRX_ID \t: ['0bc4a969dccbe3b475e9e374e53e9e3fce6dbf1e7da2fe45ca3f73ce7c84d716'\n",
      " '03ba63876abb11634b3f875ddad559ee63940573628739c3bf69e9690b5c9424'\n",
      " 'b86ab6aa560ba291acec2dd27b90f810165ff9023aab47ed84738319b19867c3' ...\n",
      " '8dc5ef52831371bafcdedc8beffe6af52736dbf7077224e14ccd912df2b56ffc'\n",
      " '41004c638bb1e1e3584902c8f22fa58953dc5b1a47ec084e54f21ce0fbe617f7'\n",
      " '1582114f0966f642cc74253d4750fce5b74f1789533724588e20055aeb787f9c'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Column Category\n",
    "check_cat = cc_df.select_dtypes(include = ['object'])\n",
    "\n",
    "for i in check_cat.columns:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()} \\n')\n",
    "    print(f'{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objetc columns: Index(['state', 'city', 'quarter', 'month', 'season', 'week_cat', 'day',\n",
      "       'limit_cat', 'geo_cat', 'fraud_status', 'cc_id', 'trx_id'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop kolom numerik\n",
    "df_obj = cc_df.select_dtypes(include = ['object'])\n",
    "print(f'objetc columns: {df_obj.columns}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object column untuk modeling: Index(['limit_cat', 'fraud_status', 'geo_cat'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# selected object columns\n",
    "filter_obj = ['limit_cat', 'fraud_status', 'geo_cat']\n",
    "selected_object = df_obj[filter_obj].columns\n",
    "\n",
    "#\n",
    "print(\"Object column untuk modeling:\", selected_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>distance</th>\n",
       "      <th>limit_cat</th>\n",
       "      <th>fraud_status</th>\n",
       "      <th>geo_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>-7642455.0</td>\n",
       "      <td>11.969568</td>\n",
       "      <td>very_low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.09</td>\n",
       "      <td>-2527299.0</td>\n",
       "      <td>15.511210</td>\n",
       "      <td>low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.21</td>\n",
       "      <td>-6508550.0</td>\n",
       "      <td>9.404226</td>\n",
       "      <td>very_high</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.41</td>\n",
       "      <td>-2534699.0</td>\n",
       "      <td>9.556419</td>\n",
       "      <td>medium</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.89</td>\n",
       "      <td>-1785659.0</td>\n",
       "      <td>8.157130</td>\n",
       "      <td>very_low</td>\n",
       "      <td>not_fraud</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_dollar_amount  time_diff_per_seconds   distance  limit_cat  \\\n",
       "0                      17.99             -7642455.0  11.969568   very_low   \n",
       "1                      12.09             -2527299.0  15.511210        low   \n",
       "2                      78.21             -6508550.0   9.404226  very_high   \n",
       "3                      74.41             -2534699.0   9.556419     medium   \n",
       "4                      54.89             -1785659.0   8.157130   very_low   \n",
       "\n",
       "  fraud_status geo_cat  \n",
       "0    not_fraud  normal  \n",
       "1    not_fraud  normal  \n",
       "2    not_fraud  normal  \n",
       "3    not_fraud  normal  \n",
       "4    not_fraud  normal  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "trans_col = selected_numeric.append(selected_object)\n",
    "\n",
    "# \n",
    "trans_col = cc_df[trans_col]\n",
    "trans_col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud_status\n",
      "not_fraud    98.25\n",
      "fraud         1.75\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# remove NaN from target\n",
    "trans_df = trans_col.dropna(subset = ['fraud_status'])\n",
    "\n",
    "# check value\n",
    "print(round(trans_df[\"fraud_status\"].value_counts(normalize = True) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# split data\n",
    "X = trans_df.drop(columns = [\"fraud_status\"]).copy()\n",
    "y = trans_df[\"fraud_status\"].copy()\n",
    "\n",
    "# convert target into numeric\n",
    "y = y.map({\"not_fraud\": 0, \"fraud\": 1})\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Columns: ['limit_cat']\n",
      "One-Hot Columns: ['geo_cat']\n",
      "Numeric Columns: ['transaction_dollar_amount', 'time_diff_per_seconds', 'distance']\n"
     ]
    }
   ],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "ordinal_set = {'limit_cat'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "ordinal_cols, one_hot_cols, numeric_cols = [], [], []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['int', 'float']:\n",
    "        numeric_cols.append(col)\n",
    "        \n",
    "    elif X[col].dtype == 'object' or X[col].dtype.name == \"category\":\n",
    "        if col in ordinal_set:\n",
    "            ordinal_cols.append(col)\n",
    "            \n",
    "        else:\n",
    "            one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Columns:\", ordinal_cols)\n",
    "print(\"One-Hot Columns:\", one_hot_cols)\n",
    "print(\"Numeric Columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_CAT \t: ['very_low' 'low' 'very_high' 'medium' 'high']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check Ordinal Columns\n",
    "for i in ordinal_cols:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()}')\n",
    "    print(f'{\"-\" * 50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menentukan urutan kategori masing-masing kolom\n",
    "oridnal_cat = [\n",
    "    [\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"],   # Urutan untuk limit_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore', \n",
    "                                        sparse_output = True, \n",
    "                                        max_categories = 50)\n",
    "ordinal_transformer = OrdinalEncoder(categories = oridnal_cat, \n",
    "                                     handle_unknown = 'use_encoded_value', \n",
    "                                     unknown_value = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer\n",
    "prep_stage_2 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_transformer, numeric_cols), \n",
    "        (\"cat\", categorical_transformer, one_hot_cols), \n",
    "        (\"ord\", ordinal_transformer, ordinal_cols)\n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = prep_stage_2.fit_transform(X_train)  # Fit & Transform Training Data\n",
    "X_test_tf = prep_stage_2.transform(X_test)  # Transform Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows X_train resample: Index(['transaction_dollar_amount', 'time_diff_per_seconds', 'distance',\n",
      "       'geo_cat_anomaly', 'geo_cat_normal', 'limit_cat'],\n",
      "      dtype='object') \n",
      "\n",
      "Total rows X_test resample: Index(['transaction_dollar_amount', 'time_diff_per_seconds', 'distance',\n",
      "       'geo_cat_anomaly', 'geo_cat_normal', 'limit_cat'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 1. Ambil Nama Kolom dari Transformer\n",
    "num_features = numeric_cols  # Kolom numerik tetap sama\n",
    "cat_features = prep_stage_2.named_transformers_[\"cat\"].get_feature_names_out(one_hot_cols)  # One-hot encoded kolom\n",
    "ord_features = ordinal_cols  # Kolom ordinal tetap sama\n",
    "\n",
    "# 2. Gabungkan Nama Kolom Baru\n",
    "transformed_columns = (list(num_features) + \n",
    "                       list(cat_features) + \n",
    "                       list(ord_features))\n",
    "\n",
    "# 3. Buat DataFrame dari Hasil Transformasi\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns = transformed_columns)\n",
    "print(f'Total rows X_train resample: {X_train_tf_df.columns} \\n')\n",
    "\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns = transformed_columns)\n",
    "print(f'Total rows X_test resample: {X_test_tf_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>distance</th>\n",
       "      <th>geo_cat_anomaly</th>\n",
       "      <th>geo_cat_normal</th>\n",
       "      <th>limit_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.316373</td>\n",
       "      <td>-0.265517</td>\n",
       "      <td>-0.174716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.850314</td>\n",
       "      <td>0.539346</td>\n",
       "      <td>-0.172776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.790372</td>\n",
       "      <td>-1.797245</td>\n",
       "      <td>-0.174435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.695941</td>\n",
       "      <td>-2.187629</td>\n",
       "      <td>-0.169949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259803</td>\n",
       "      <td>-0.863683</td>\n",
       "      <td>-0.170490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_dollar_amount  time_diff_per_seconds  distance  \\\n",
       "0                  -0.316373              -0.265517 -0.174716   \n",
       "1                  -0.850314               0.539346 -0.172776   \n",
       "2                  -0.790372              -1.797245 -0.174435   \n",
       "3                  -0.695941              -2.187629 -0.169949   \n",
       "4                   0.259803              -0.863683 -0.170490   \n",
       "\n",
       "   geo_cat_anomaly  geo_cat_normal  limit_cat  \n",
       "0              0.0             1.0        1.0  \n",
       "1              0.0             1.0        1.0  \n",
       "2              0.0             1.0        1.0  \n",
       "3              0.0             1.0        2.0  \n",
       "4              0.0             1.0        1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_dollar_amount</th>\n",
       "      <th>time_diff_per_seconds</th>\n",
       "      <th>distance</th>\n",
       "      <th>geo_cat_anomaly</th>\n",
       "      <th>geo_cat_normal</th>\n",
       "      <th>limit_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098739</td>\n",
       "      <td>-0.703509</td>\n",
       "      <td>-0.172181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.348349</td>\n",
       "      <td>-0.196263</td>\n",
       "      <td>-0.173304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.766949</td>\n",
       "      <td>-0.458393</td>\n",
       "      <td>-0.168484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.162931</td>\n",
       "      <td>1.083388</td>\n",
       "      <td>-0.174020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.571632</td>\n",
       "      <td>-0.896323</td>\n",
       "      <td>-0.168833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_dollar_amount  time_diff_per_seconds  distance  \\\n",
       "0                  -0.098739              -0.703509 -0.172181   \n",
       "1                   2.348349              -0.196263 -0.173304   \n",
       "2                  -0.766949              -0.458393 -0.168484   \n",
       "3                  -1.162931               1.083388 -0.174020   \n",
       "4                  -0.571632              -0.896323 -0.168833   \n",
       "\n",
       "   geo_cat_anomaly  geo_cat_normal  limit_cat  \n",
       "0              0.0             1.0        1.0  \n",
       "1              0.0             1.0        1.0  \n",
       "2              0.0             1.0        1.0  \n",
       "3              0.0             1.0        0.0  \n",
       "4              0.0             1.0        2.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # over-sampling\n",
    "# smote = SMOTE(sampling_strategy = 0.8, \n",
    "#               k_neighbors = NearestNeighbors(n_jobs = -1), \n",
    "#               random_state = 42)\n",
    "\n",
    "# # under-sampling\n",
    "# tomek = TomekLinks(sampling_strategy = 'majority')\n",
    "\n",
    "# # resampling\n",
    "# sampling = SMOTETomek(smote = smote, \n",
    "#                       tomek = tomek, \n",
    "#                       random_state = 42)\n",
    "\n",
    "# X_train_resample, y_train_resample = sampling.fit_resample(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # over-sampling\n",
    "# smote = SMOTE(sampling_strategy = 0.4, \n",
    "#               k_neighbors = NearestNeighbors(n_jobs = -1), \n",
    "#               random_state = 42)\n",
    "\n",
    "# # under-sampling\n",
    "# tomek = TomekLinks(sampling_strategy = 'auto')\n",
    "\n",
    "# # resampling\n",
    "# sampling = SMOTETomek(smote = smote, \n",
    "#                       tomek = tomek, \n",
    "#                       random_state = 42)\n",
    "\n",
    "# X_train_resample, y_train_resample = sampling.fit_resample(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over-sampling\n",
    "smote = SMOTE(sampling_strategy = 0.3, \n",
    "              k_neighbors = NearestNeighbors(n_jobs = -1), \n",
    "              random_state = 42)\n",
    "\n",
    "# under-sampling\n",
    "tomek = TomekLinks(sampling_strategy = 'not majority')\n",
    "\n",
    "# resampling\n",
    "sampling = SMOTETomek(smote = smote, \n",
    "                      tomek = tomek, \n",
    "                      random_state = 42)\n",
    "\n",
    "X_train_resample, y_train_resample = sampling.fit_resample(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTETomek:\n",
      "fraud_status\n",
      "0    98.246016\n",
      "1     1.753984\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "After SMOTETomek:\n",
      "fraud_status\n",
      "0    76.923183\n",
      "1    23.076817\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Before After Data Distribution\n",
    "print(\"Before SMOTETomek:\")\n",
    "print(y_train.value_counts(normalize = True) * 100)\n",
    "\n",
    "print(\"\\nAfter SMOTETomek:\")\n",
    "print(y_train_resample.value_counts(normalize = True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leak Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transaction_dollar_amount   -0.017419\n",
      "time_diff_per_seconds       -0.287827\n",
      "distance                     0.917634\n",
      "geo_cat_anomaly              1.000000\n",
      "geo_cat_normal              -1.000000\n",
      "limit_cat                   -0.023075\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Konversi X_train_resample ke DataFrame dengan nama kolom yang sama seperti sebelum resampling\n",
    "X_train_leak = pd.DataFrame(X_train_resample, columns = X_train_tf_df.columns)\n",
    "\n",
    "# Cek korelasi antara fitur dan label\n",
    "print(X_train_leak.corrwith(pd.Series(y_train_resample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance           0.917634\n",
      "geo_cat_anomaly    1.000000\n",
      "geo_cat_normal    -1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation_values = X_train_leak.corrwith(pd.Series(y_train_resample))\n",
    "high_correlation_features = correlation_values[correlation_values.abs() > 0.9]\n",
    "\n",
    "print(high_correlation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 289884 entries, 0 to 289883\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   transaction_dollar_amount  289884 non-null  float64\n",
      " 1   time_diff_per_seconds      289884 non-null  float64\n",
      " 2   limit_cat                  289884 non-null  float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 6.6 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_leak = X_train_leak.drop(columns = high_correlation_features.index)\n",
    "X_train_leak.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transaction_dollar_amount    0.002608\n",
      "time_diff_per_seconds        0.018127\n",
      "distance                     0.017696\n",
      "geo_cat_anomaly              0.008779\n",
      "geo_cat_normal              -0.008779\n",
      "limit_cat                   -0.017275\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Konversi X_train_resample ke DataFrame dengan nama kolom yang sama seperti sebelum resampling\n",
    "X_test_leak = pd.DataFrame(X_test_tf_df, columns = X_test_tf_df.columns)\n",
    "\n",
    "# Cek korelasi antara fitur dan label\n",
    "print(X_test_leak.corrwith(pd.Series(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56743 entries, 0 to 56742\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   transaction_dollar_amount  56743 non-null  float64\n",
      " 1   time_diff_per_seconds      56743 non-null  float64\n",
      " 2   limit_cat                  56743 non-null  float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_test_leak = X_test_leak.drop(columns = high_correlation_features.index, errors = \"ignore\")\n",
    "X_test_leak.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename var train\n",
    "X_train_mod = X_train_leak.copy()\n",
    "y_train_mod = y_train_resample.copy()\n",
    "\n",
    "# Rename var test\n",
    "X_test_mod = X_test_leak.copy()\n",
    "y_test_mod = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(class_weight = \"balanced\", \n",
    "                                 solver = \"liblinear\", \n",
    "                                 random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators = 200, \n",
    "                                   max_depth = 10, \n",
    "                                   class_weight = \"balanced\", \n",
    "                                   random_state = 42, \n",
    "                                   n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangani kasus ZeroDivisionError jika kelas minoritas tidak ada di y_train\n",
    "if np.sum(y_train_resample == 1) == 0:\n",
    "    scale_pos_weight = 1\n",
    "    \n",
    "else:\n",
    "    scale_pos_weight = np.sum(y_train_resample == 0) / np.sum(y_train_resample == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(scale_pos_weight = scale_pos_weight, \n",
    "                          eval_metric = \"logloss\", \n",
    "                          random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm_model = LGBMClassifier(is_unbalance = True, \n",
    "                            force_col_wise = True, \n",
    "                            max_depth = 10,  # Menambah kedalaman pohon \n",
    "                            min_data_in_leaf = 10,  # Menghindari split yang tidak berguna \n",
    "                            eval_metric = \"logloss\",  # Metode evaluasi yang lebih jelas \n",
    "                            verbose = -1,  # Mengurangi log yang berlebihan \n",
    "                            random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "catb_model = CatBoostClassifier(auto_class_weights = 'Balanced', \n",
    "                                verbose = 0, \n",
    "                                random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan semua pipeline dalam dictionary\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": logreg_model,\n",
    "    \"Random Forest\": forest_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"LightGBM\": lbgm_model,\n",
    "    \"CatBoost\": catb_model\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_roc_auc_test = 0\n",
    "best_score_diff = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluasi Model: Logistic Regression\n",
      "==================================================\n",
      "Optimal Threshold Found: 0.5\n",
      "\n",
      "=== Classification Report (TRAIN - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.63      0.72    222988\n",
      "           1       0.33      0.60      0.42     66896\n",
      "\n",
      "    accuracy                           0.63    289884\n",
      "   macro avg       0.58      0.61      0.57    289884\n",
      "weighted avg       0.72      0.63      0.65    289884\n",
      "\n",
      "ROC-AUC Score (Train): 0.705\n",
      "\n",
      "=== Classification Report (TEST - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.64      0.77     55748\n",
      "           1       0.03      0.57      0.05       995\n",
      "\n",
      "    accuracy                           0.64     56743\n",
      "   macro avg       0.51      0.60      0.41     56743\n",
      "weighted avg       0.97      0.64      0.76     56743\n",
      "\n",
      "ROC-AUC Score (Test): 0.698\n",
      "================================================== \n",
      "\n",
      " Evaluasi Model: Random Forest\n",
      "==================================================\n",
      "Optimal Threshold Found: 0.629\n",
      "\n",
      "=== Classification Report (TRAIN - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85    222988\n",
      "           1       0.50      0.56      0.53     66896\n",
      "\n",
      "    accuracy                           0.77    289884\n",
      "   macro avg       0.68      0.70      0.69    289884\n",
      "weighted avg       0.78      0.77      0.78    289884\n",
      "\n",
      "ROC-AUC Score (Train): 0.837\n",
      "\n",
      "=== Classification Report (TEST - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90     55748\n",
      "           1       0.04      0.45      0.08       995\n",
      "\n",
      "    accuracy                           0.82     56743\n",
      "   macro avg       0.52      0.64      0.49     56743\n",
      "weighted avg       0.97      0.82      0.89     56743\n",
      "\n",
      "ROC-AUC Score (Test): 0.792\n",
      "================================================== \n",
      "\n",
      " Evaluasi Model: XGBoost\n",
      "==================================================\n",
      "Optimal Threshold Found: 0.568\n",
      "\n",
      "=== Classification Report (TRAIN - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.75      0.83    222988\n",
      "           1       0.50      0.84      0.63     66896\n",
      "\n",
      "    accuracy                           0.77    289884\n",
      "   macro avg       0.72      0.80      0.73    289884\n",
      "weighted avg       0.84      0.77      0.79    289884\n",
      "\n",
      "ROC-AUC Score (Train): 0.886\n",
      "\n",
      "=== Classification Report (TEST - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.74      0.85     55748\n",
      "           1       0.04      0.58      0.07       995\n",
      "\n",
      "    accuracy                           0.74     56743\n",
      "   macro avg       0.51      0.66      0.46     56743\n",
      "weighted avg       0.97      0.74      0.83     56743\n",
      "\n",
      "ROC-AUC Score (Test): 0.789\n",
      "================================================== \n",
      "\n",
      " Evaluasi Model: LightGBM\n",
      "==================================================\n",
      "Optimal Threshold Found: 0.611\n",
      "\n",
      "=== Classification Report (TRAIN - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.77      0.84    222988\n",
      "           1       0.50      0.77      0.61     66896\n",
      "\n",
      "    accuracy                           0.77    289884\n",
      "   macro avg       0.71      0.77      0.72    289884\n",
      "weighted avg       0.82      0.77      0.78    289884\n",
      "\n",
      "ROC-AUC Score (Train): 0.869\n",
      "\n",
      "=== Classification Report (TEST - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86     55748\n",
      "           1       0.04      0.55      0.07       995\n",
      "\n",
      "    accuracy                           0.76     56743\n",
      "   macro avg       0.51      0.66      0.47     56743\n",
      "weighted avg       0.97      0.76      0.85     56743\n",
      "\n",
      "ROC-AUC Score (Test): 0.795\n",
      "================================================== \n",
      "\n",
      " Evaluasi Model: CatBoost\n",
      "==================================================\n",
      "Optimal Threshold Found: 0.559\n",
      "\n",
      "=== Classification Report (TRAIN - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83    222988\n",
      "           1       0.50      0.85      0.63     66896\n",
      "\n",
      "    accuracy                           0.77    289884\n",
      "   macro avg       0.72      0.80      0.73    289884\n",
      "weighted avg       0.84      0.77      0.79    289884\n",
      "\n",
      "ROC-AUC Score (Train): 0.889\n",
      "\n",
      "=== Classification Report (TEST - Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.74      0.85     55748\n",
      "           1       0.04      0.58      0.07       995\n",
      "\n",
      "    accuracy                           0.74     56743\n",
      "   macro avg       0.51      0.66      0.46     56743\n",
      "weighted avg       0.97      0.74      0.83     56743\n",
      "\n",
      "ROC-AUC Score (Test): 0.789\n",
      "================================================== \n",
      "\n",
      " Model Terbaik: LightGBM dengan ROC-AUC Test tertinggi: 0.795 dan perbedaan ROC-AUC: 0.074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Evaluasi semua model\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\" Evaluasi Model: {name}\")\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Training model\n",
    "    # Tangani kasus khusus untuk CatBoost\n",
    "    if name == \"CatBoost\":\n",
    "        pipe.fit(X_train_mod, y_train_mod, verbose = False)\n",
    "\n",
    "    else:\n",
    "        pipe.fit(X_train_mod, y_train_mod)\n",
    "    \n",
    "    # === Train Evaluation ===\n",
    "    y_train_pred_proba = pipe.predict_proba(X_train_mod)[:, 1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_train_mod, y_train_pred_proba)\n",
    "    \n",
    "    # Sesuaikan ukuran threshold\n",
    "    thresholds = np.append(thresholds, 1.0)\n",
    "    valid_idx = (precisions >= 0.5) & (recalls >= 0.5)\n",
    "    valid_thresholds = thresholds[valid_idx]\n",
    "    best_threshold = valid_thresholds[0] if len(valid_thresholds) > 0 else 0.5\n",
    "    best_threshold = round(best_threshold, 3)\n",
    "    print(f\"Optimal Threshold Found: {best_threshold}\")\n",
    "    \n",
    "    # Prediksi ulang dengan threshold optimal\n",
    "    y_train_pred_custom = (y_train_pred_proba >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Classification Report (TRAIN - Optimized Threshold) ===\")\n",
    "    print(classification_report(y_train_mod, y_train_pred_custom))\n",
    "    roc_auc_train = roc_auc_score(y_train_mod, y_train_pred_proba)\n",
    "    print(f\"ROC-AUC Score (Train): {roc_auc_train:.3f}\")\n",
    "    \n",
    "    # === Test Evaluation ===\n",
    "    y_test_pred_proba = pipe.predict_proba(X_test_mod)[:, 1]\n",
    "    y_test_pred_custom = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Classification Report (TEST - Optimized Threshold) ===\")\n",
    "    print(classification_report(y_test_mod, y_test_pred_custom))\n",
    "    roc_auc_test = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "    print(f\"ROC-AUC Score (Test): {roc_auc_test:.3f}\")\n",
    "    print('=' * 50, '\\n')\n",
    "    \n",
    "    # Evaluasi model terbaik berdasarkan kombinasi nilai ROC-AUC Test dan perbedaan dengan Train\n",
    "    score_diff = abs(roc_auc_train - roc_auc_test)\n",
    "    \n",
    "    # Perbaikan: Pilih model dengan ROC-AUC Test tertinggi, atau jika sama, dengan score_diff terkecil\n",
    "    if roc_auc_test > best_roc_auc_test or (roc_auc_test == best_roc_auc_test and score_diff < best_score_diff):\n",
    "        best_model = pipe\n",
    "        best_model_name = name\n",
    "        best_roc_auc_test = roc_auc_test\n",
    "        best_score_diff = score_diff\n",
    "\n",
    "print(f\" Model Terbaik: {best_model_name} dengan ROC-AUC Test tertinggi: {best_roc_auc_test:.3f} dan perbedaan ROC-AUC: {best_score_diff:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report (Voting Classifier) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.74     55748\n",
      "           1       0.04      0.86      0.07       995\n",
      "\n",
      "    accuracy                           0.59     56743\n",
      "   macro avg       0.52      0.72      0.40     56743\n",
      "weighted avg       0.98      0.59      0.72     56743\n",
      "\n",
      "ROC-AUC Score (Voting Classifier): 0.789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Membuat Voting Classifier dengan model terbaik\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', forest_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lbgm_model),\n",
    "        ('catb', catb_model)\n",
    "    ],\n",
    "    voting = 'soft'  # Menggunakan probabilitas\n",
    ")\n",
    "\n",
    "# Training Voting Classifier\n",
    "voting_clf.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "# Evaluasi pada Test Set\n",
    "y_test_pred_proba = voting_clf.predict_proba(X_test_mod)[:, 1]\n",
    "\n",
    "# Optimasi threshold menggunakan Precision-Recall Curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_mod, y_test_pred_proba)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "valid_idx = (precisions >= 0.5) & (recalls >= 0.5)\n",
    "best_threshold = thresholds[valid_idx][0] if len(thresholds[valid_idx]) > 0 else 0.5\n",
    "\n",
    "# Prediksi menggunakan threshold optimal\n",
    "y_test_pred_custom = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluasi Voting Classifier\n",
    "print(\"\\n=== Classification Report (Voting Classifier) ===\")\n",
    "print(classification_report(y_test_mod, y_test_pred_custom))\n",
    "\n",
    "roc_auc_voting = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "print(f\"ROC-AUC Score (Voting Classifier): {roc_auc_voting:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Terbaik untuk Cross-Validation: LightGBM dengan ROC-AUC: 0.795\n"
     ]
    }
   ],
   "source": [
    "# Menentukan model terbaik antara pipeline terbaik dan voting classifier\n",
    "final_model = None\n",
    "final_model_name = \"\"\n",
    "\n",
    "if roc_auc_voting > best_roc_auc_test:\n",
    "    final_model = voting_clf\n",
    "    final_model_name = \"voting_clf\"\n",
    "    final_roc_auc = roc_auc_voting\n",
    "\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_model_name = best_model_name\n",
    "    final_roc_auc = best_roc_auc_test\n",
    "\n",
    "print(f\"\\n Model Terbaik untuk Cross-Validation: {final_model_name} dengan ROC-AUC: {final_roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Optimal CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === 5 Cross Validation pada Model Terbaik ===\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(best_model, X_train_mod, y_train_mod, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# print(f\"\\n Cross Validation ROC-AUC Scores: {cv_scores}\")\n",
    "# print(f\" Mean ROC-AUC: {np.mean(cv_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mencari Nilai CV Optimal...\n",
      "\n",
      "Evaluasi dengan cv = 3\n",
      "ROC-AUC rata-rata: 0.863 (dengan cv = 3)\n",
      "\n",
      "Evaluasi dengan cv = 5\n",
      "ROC-AUC rata-rata: 0.863 (dengan cv = 5)\n",
      "\n",
      "Evaluasi dengan cv = 7\n",
      "ROC-AUC rata-rata: 0.864 (dengan cv = 7)\n",
      "\n",
      "Evaluasi dengan cv = 10\n",
      "ROC-AUC rata-rata: 0.863 (dengan cv = 10)\n",
      "\n",
      " Nilai CV Optimal: 7 dengan ROC-AUC: 0.864\n",
      "\n",
      " Evaluasi Model Terbaik dengan CV Optimal...\n",
      "\n",
      " Cross Validation ROC-AUC Scores: [0.86267059 0.86590366 0.86792473 0.8631455  0.86241353 0.85880378\n",
      " 0.86302913]\n",
      " Mean ROC-AUC: 0.863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Daftar nilai CV yang ingin diuji\n",
    "cv_values = [3, 5, 7, 10]\n",
    "best_cv = None\n",
    "best_cv_score = 0\n",
    "\n",
    "print(\"\\n Mencari Nilai CV Optimal...\")\n",
    "\n",
    "for cv in cv_values:\n",
    "    print(f\"\\nEvaluasi dengan cv = {cv}\")\n",
    "    \n",
    "    # Hitung skor ROC-AUC dengan cross-validation menggunakan final_model\n",
    "    scores = cross_val_score(final_model, X_train_mod, y_train_mod, \n",
    "                             cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"ROC-AUC rata-rata: {mean_score:.3f} (dengan cv = {cv})\")\n",
    "    \n",
    "    # Simpan nilai CV terbaik\n",
    "    if mean_score > best_cv_score:\n",
    "        best_cv_score = mean_score\n",
    "        best_cv = cv\n",
    "\n",
    "print(f\"\\n Nilai CV Optimal: {best_cv} dengan ROC-AUC: {best_cv_score:.3f}\")\n",
    "\n",
    "# === 5 Evaluasi Ulang Cross Validation pada Model Terbaik ===\n",
    "print(\"\\n Evaluasi Model Terbaik dengan CV Optimal...\")\n",
    "cv = StratifiedKFold(n_splits=best_cv, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(final_model, X_train_mod, y_train_mod, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"\\n Cross Validation ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\" Mean ROC-AUC: {np.mean(cv_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2888956130.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[47], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    sam =\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sam = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# if final_model_name != \"Voting Classifier\":\n",
    "#     param_grid = {\n",
    "#         \"Random Forest\": {\n",
    "#             'n_estimators': [100, 200, 300],\n",
    "#             'max_depth': [10, 20, None],\n",
    "#             'min_samples_split': [2, 5, 10]\n",
    "#         },\n",
    "#         \"XGBoost\": {\n",
    "#             'n_estimators': [100, 200, 300],\n",
    "#             'learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'max_depth': [3, 6, 10]\n",
    "#         },\n",
    "#         \"LightGBM\": {\n",
    "#             'num_leaves': [31, 50, 100],\n",
    "#             'learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'n_estimators': [100, 200, 300]\n",
    "#         },\n",
    "#         \"CatBoost\": {\n",
    "#             'iterations': [100, 200, 300],\n",
    "#             'learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'depth': [4, 6, 10]\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     grid_params = param_grid.get(final_model_name, {})\n",
    "\n",
    "#     grid_search = GridSearchCV(final_model, grid_params, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "#     grid_search.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "#     final_model = grid_search.best_estimator_\n",
    "\n",
    "#     print(f\"\\n Hyperparameter Terbaik untuk {final_model_name}: {grid_search.best_params_}\")\n",
    "#     print(f\" Best ROC-AUC Score: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "halving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih model yang akan di-tuning\n",
    "if final_model_name == \"Random Forest\":\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"XGBoost\":\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 10]\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"LightGBM\":\n",
    "    param_dist = {\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300]\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"CatBoost\":\n",
    "    param_dist = {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'depth': [4, 6, 10]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "halving_search = HalvingRandomSearchCV(\n",
    "    final_model, param_distributions=param_dist, \n",
    "    factor=2, scoring='roc_auc', cv=3, \n",
    "    verbose=1, n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "halving_search.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "final_model = halving_search.best_estimator_\n",
    "\n",
    "print(f\"\\n Hyperparameter Terbaik ({final_model_name}): {halving_search.best_params_}\")\n",
    "print(f\" Best ROC-AUC Score: {halving_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optuna (bayesian optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from optuna.integration import SklearnPruningCallback\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {}\n",
    "    \n",
    "#     if final_model_name == \"Random Forest\":\n",
    "#         params = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "#             'min_samples_split': trial.suggest_int('min_samples_split', 2, 10)\n",
    "#         }\n",
    "\n",
    "#     elif final_model_name == \"XGBoost\":\n",
    "#         params = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "#             'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "#         }\n",
    "\n",
    "#     elif final_model_name == \"LightGBM\":\n",
    "#         params = {\n",
    "#             'num_leaves': trial.suggest_int('num_leaves', 31, 200),\n",
    "#             'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 500)\n",
    "#         }\n",
    "\n",
    "#     elif final_model_name == \"CatBoost\":\n",
    "#         params = {\n",
    "#             'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "#             'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "#             'depth': trial.suggest_int('depth', 4, 10)\n",
    "#         }\n",
    "\n",
    "#     model = final_model.set_params(**params)\n",
    "    \n",
    "#     cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train_mod, y_train_mod, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "#     return np.mean(scores)\n",
    "\n",
    "# # Optuna study\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# final_model = final_model.set_params(**best_params)\n",
    "\n",
    "# print(f\"\\n Hyperparameter Terbaik ({final_model_name}): {best_params}\")\n",
    "# print(f\" Best ROC-AUC Score: {study.best_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Prediksi pada Test Set\n",
    "# y_test_pred_proba = final_model.predict_proba(X_test_mod)[:, 1]\n",
    "# y_test_pred = (y_test_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# #  Classification Report\n",
    "# print(\"\\n=== Classification Report ===\")\n",
    "# print(classification_report(y_test_mod, y_test_pred))\n",
    "\n",
    "# #  ROC-AUC Score\n",
    "# roc_auc_final = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "# print(f\" Final ROC-AUC Score: {roc_auc_final:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# # 1 ROC Curve\n",
    "# fpr, tpr, _ = roc_curve(y_test_mod, y_test_pred_proba)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_final:.3f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')  \n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(f\"ROC Curve - {final_model_name}\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # 2 Confusion Matrix\n",
    "# cm = confusion_matrix(y_test_mod, y_test_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title(f\"Confusion Matrix - {final_model_name}\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
