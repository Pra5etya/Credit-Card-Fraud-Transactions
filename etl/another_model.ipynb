{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library / Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "# complex math\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# data preparation\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "# data blueprint\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# data modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# data cross-validation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# data metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "\n",
    "# data tuning   \n",
    "from itertools import product\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# pickle and .env\n",
    "from dotenv import dotenv_values\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_round(x, pos): \n",
    "    if abs(x) >= 1e9: \n",
    "        return f'{x/1e9} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x}'\n",
    "    \n",
    "def val_round(x):\n",
    "    if abs(x) >= 1e9:\n",
    "        return f'{x/1e9:.2f} B'\n",
    "    \n",
    "    elif abs(x) >= 1e6:\n",
    "        return f'{x/1e6:.2f} M'\n",
    "    \n",
    "    elif abs(x) >= 1e3:\n",
    "        return f'{x/1e3:.2f} K'\n",
    "    \n",
    "    else:\n",
    "        return f'{x:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk konversi tipe data\n",
    "def convert_object_columns_to_numeric(df):\n",
    "    for col in df.select_dtypes(include = ['object']).columns:  \n",
    "        try:\n",
    "            # Cek apakah semua nilai bisa dikonversi ke float\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "            \n",
    "            # Jika bisa, ubah ke int jika semua nilai adalah bilangan bulat\n",
    "            if all(df[col] % 1 == 0):  # Cek apakah semua nilai adalah bilangan bulat\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "        except ValueError:\n",
    "            pass  # Jika ada nilai non-angka, biarkan tetap object\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "share = {**dotenv_values('../.env.shared')} \n",
    "\n",
    "# read pickle\n",
    "with open(share['CLEAN_DATA'], 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "cc_df = pd.DataFrame(loaded_data)\n",
    "cc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and Irrelevant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Threshold Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Drop kolom non-numerik\n",
    "df_numeric = cc_df.select_dtypes(include = ['number'])\n",
    "print(f'numeric columns: {df_numeric.columns}\\n')\n",
    "\n",
    "# Inisialisasi VarianceThreshold (misalnya, ambang batas 0.01)\n",
    "selector = VarianceThreshold(threshold = 0.01)\n",
    "df_var_selected = selector.fit_transform(df_numeric)\n",
    "\n",
    "# Fitur yang dipertahankan\n",
    "selected_features = df_numeric.columns[selector.get_support()]\n",
    "print(\"Fitur yang dipertahankan:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleceted numeric columns\n",
    "filter_numeric = ['credit_card', 'long', 'lat', 'zipcode', 'credit_card_limit', 'prev_long', 'prev_lat']\n",
    "selected_numeric = selected_features.drop(filter_numeric)\n",
    "\n",
    "#\n",
    "print(\"Numeric column untuk modeling:\", selected_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Relevant Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column Category\n",
    "check_cat = cc_df.select_dtypes(include = ['object'])\n",
    "\n",
    "for i in check_cat.columns:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()} \\n')\n",
    "    print(f'{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop kolom numerik\n",
    "df_obj = cc_df.select_dtypes(include = ['object'])\n",
    "print(f'objetc columns: {df_obj.columns}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected object columns\n",
    "filter_obj = ['limit_cat', 'fraud_status', 'geo_cat']\n",
    "selected_object = df_obj[filter_obj].columns\n",
    "\n",
    "#\n",
    "print(\"Object column untuk modeling:\", selected_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "trans_col = selected_numeric.append(selected_object)\n",
    "\n",
    "# \n",
    "trans_col = cc_df[trans_col]\n",
    "trans_col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN from target\n",
    "trans_df = trans_col.dropna(subset = ['fraud_status'])\n",
    "\n",
    "# check value\n",
    "print(round(trans_df[\"fraud_status\"].value_counts(normalize = True) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# split data\n",
    "X = trans_df.drop(columns = [\"fraud_status\"]).copy()\n",
    "y = trans_df[\"fraud_status\"].copy()\n",
    "\n",
    "# convert target into numeric\n",
    "y = y.map({\"not_fraud\": 0, \"fraud\": 1})\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar kolom untuk label encoding (kolom ordinal)\n",
    "ordinal_set = {'limit_cat'}\n",
    "\n",
    "# Inisialisasi list untuk menyimpan kolom yang telah dikelompokkan\n",
    "ordinal_cols, one_hot_cols, numeric_cols = [], [], []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['int', 'float']:\n",
    "        numeric_cols.append(col)\n",
    "        \n",
    "    elif X[col].dtype == 'object' or X[col].dtype.name == \"category\":\n",
    "        if col in ordinal_set:\n",
    "            ordinal_cols.append(col)\n",
    "            \n",
    "        else:\n",
    "            one_hot_cols.append(col)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"Ordinal Columns:\", ordinal_cols)\n",
    "print(\"One-Hot Columns:\", one_hot_cols)\n",
    "print(\"Numeric Columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ordinal Columns\n",
    "for i in ordinal_cols:\n",
    "    print(f'{i.upper()} \\t: {check_cat[i].unique()}')\n",
    "    print(f'{\"-\" * 50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menentukan urutan kategori masing-masing kolom\n",
    "oridnal_cat = [\n",
    "    [\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"],   # Urutan untuk limit_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore', \n",
    "                                        sparse_output = True, \n",
    "                                        max_categories = 50)\n",
    "ordinal_transformer = OrdinalEncoder(categories = oridnal_cat, \n",
    "                                     handle_unknown = 'use_encoded_value', \n",
    "                                     unknown_value = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformer\n",
    "prep_stage_2 = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"num\", numerical_transformer, numeric_cols), \n",
    "        (\"cat\", categorical_transformer, one_hot_cols), \n",
    "        (\"ord\", ordinal_transformer, ordinal_cols)\n",
    "    ], remainder = \"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = prep_stage_2.fit_transform(X_train)  # Fit & Transform Training Data\n",
    "X_test_tf = prep_stage_2.transform(X_test)  # Transform Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ambil Nama Kolom dari Transformer\n",
    "num_features = numeric_cols  # Kolom numerik tetap sama\n",
    "cat_features = prep_stage_2.named_transformers_[\"cat\"].get_feature_names_out(one_hot_cols)  # One-hot encoded kolom\n",
    "ord_features = ordinal_cols  # Kolom ordinal tetap sama\n",
    "\n",
    "# 2. Gabungkan Nama Kolom Baru\n",
    "transformed_columns = (list(num_features) + \n",
    "                       list(cat_features) + \n",
    "                       list(ord_features))\n",
    "\n",
    "# 3. Buat DataFrame dari Hasil Transformasi\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns = transformed_columns)\n",
    "print(f'Total rows X_train resample: {X_train_tf_df.columns} \\n')\n",
    "\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns = transformed_columns)\n",
    "print(f'Total rows X_test resample: {X_test_tf_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over-sampling\n",
    "smote = SMOTE(sampling_strategy = 0.3, \n",
    "              k_neighbors = NearestNeighbors(n_jobs = -1), \n",
    "              random_state = 42)\n",
    "\n",
    "# under-sampling\n",
    "tomek = TomekLinks(sampling_strategy = 'not majority')\n",
    "\n",
    "# resampling\n",
    "sampling = SMOTETomek(smote = smote, \n",
    "                      tomek = tomek, \n",
    "                      random_state = 42)\n",
    "\n",
    "X_train_resample, y_train_resample = sampling.fit_resample(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before After Data Distribution\n",
    "print(\"Before SMOTETomek:\")\n",
    "print(y_train.value_counts(normalize = True) * 100)\n",
    "\n",
    "print(\"\\nAfter SMOTETomek:\")\n",
    "print(y_train_resample.value_counts(normalize = True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leak Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi X_train_resample ke DataFrame dengan nama kolom yang sama seperti sebelum resampling\n",
    "X_train_leak = pd.DataFrame(X_train_resample, columns = X_train_tf_df.columns)\n",
    "\n",
    "# Cek korelasi antara fitur dan label\n",
    "print(X_train_leak.corrwith(pd.Series(y_train_resample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check High Correlations\n",
    "correlation_values = X_train_leak.corrwith(pd.Series(y_train_resample))\n",
    "high_correlation_features = correlation_values[correlation_values.abs() > 0.9]\n",
    "\n",
    "print(high_correlation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_leak = X_train_leak.drop(columns = high_correlation_features.index)\n",
    "X_train_leak.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi X_train_resample ke DataFrame dengan nama kolom yang sama seperti sebelum resampling\n",
    "X_test_leak = pd.DataFrame(X_test_tf_df, columns = X_test_tf_df.columns)\n",
    "\n",
    "# Cek korelasi antara fitur dan label\n",
    "print(X_test_leak.corrwith(pd.Series(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_leak = X_test_leak.drop(columns = high_correlation_features.index, errors = \"ignore\")\n",
    "X_test_leak.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename var train\n",
    "X_train_mod = X_train_leak.copy()\n",
    "y_train_mod = y_train_resample.copy()\n",
    "\n",
    "# Rename var test\n",
    "X_test_mod = X_test_leak.copy()\n",
    "y_test_mod = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(class_weight = \"balanced\", \n",
    "                                 solver = \"liblinear\", \n",
    "                                 random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(n_estimators = 200, \n",
    "                                   max_depth = 10, \n",
    "                                   class_weight = \"balanced\", \n",
    "                                   random_state = 42, \n",
    "                                   n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangani kasus ZeroDivisionError jika kelas minoritas tidak ada di y_train\n",
    "if np.sum(y_train_resample == 1) == 0:\n",
    "    scale_pos_weight = 1\n",
    "    \n",
    "else:\n",
    "    scale_pos_weight = np.sum(y_train_resample == 0) / np.sum(y_train_resample == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(scale_pos_weight = scale_pos_weight, \n",
    "                          eval_metric = \"logloss\", \n",
    "                          random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbgm_model = LGBMClassifier(is_unbalance = True, \n",
    "                            force_col_wise = True, \n",
    "                            max_depth = 10,  # Menambah kedalaman pohon \n",
    "                            min_data_in_leaf = 10,  # Menghindari split yang tidak berguna \n",
    "                            eval_metric = \"logloss\",  # Metode evaluasi yang lebih jelas \n",
    "                            verbose = -1,  # Mengurangi log yang berlebihan \n",
    "                            random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catb_model = CatBoostClassifier(auto_class_weights = 'Balanced', \n",
    "                                verbose = 0, \n",
    "                                random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan semua pipeline dalam dictionary\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": logreg_model,\n",
    "    \"Random Forest\": forest_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"LightGBM\": lbgm_model,\n",
    "    \"CatBoost\": catb_model\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_roc_auc_test = 0\n",
    "best_score_diff = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi semua model\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"🔹 Evaluasi Model: {name}\")\n",
    "    print('=' * 50)\n",
    "    \n",
    "    # Training model\n",
    "    # Tangani kasus khusus untuk CatBoost\n",
    "    if name == \"CatBoost\":\n",
    "        pipe.fit(X_train_mod, y_train_mod, verbose = False)\n",
    "\n",
    "    else:\n",
    "        pipe.fit(X_train_mod, y_train_mod)\n",
    "    \n",
    "    # === Train Evaluation ===\n",
    "    y_train_pred_proba = pipe.predict_proba(X_train_mod)[:, 1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_train_mod, y_train_pred_proba)\n",
    "    \n",
    "    # Sesuaikan ukuran threshold\n",
    "    thresholds = np.append(thresholds, 1.0)\n",
    "    valid_idx = (precisions >= 0.5) & (recalls >= 0.5)\n",
    "    valid_thresholds = thresholds[valid_idx]\n",
    "    best_threshold = valid_thresholds[0] if len(valid_thresholds) > 0 else 0.5\n",
    "    best_threshold = round(best_threshold, 3)\n",
    "    print(f\"Optimal Threshold Found: {best_threshold}\")\n",
    "    \n",
    "    # Prediksi ulang dengan threshold optimal\n",
    "    y_train_pred_custom = (y_train_pred_proba >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Classification Report (TRAIN - Optimized Threshold) ===\")\n",
    "    print(classification_report(y_train_mod, y_train_pred_custom))\n",
    "    roc_auc_train = roc_auc_score(y_train_mod, y_train_pred_proba)\n",
    "    print(f\"ROC-AUC Score (Train): {roc_auc_train:.3f}\")\n",
    "    \n",
    "    # === Test Evaluation ===\n",
    "    y_test_pred_proba = pipe.predict_proba(X_test_mod)[:, 1]\n",
    "    y_test_pred_custom = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Classification Report (TEST - Optimized Threshold) ===\")\n",
    "    print(classification_report(y_test_mod, y_test_pred_custom))\n",
    "    roc_auc_test = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "    print(f\"ROC-AUC Score (Test): {roc_auc_test:.3f}\")\n",
    "    print('=' * 50, '\\n')\n",
    "    \n",
    "    # Evaluasi model terbaik berdasarkan kombinasi nilai ROC-AUC Test dan perbedaan dengan Train\n",
    "    score_diff = abs(roc_auc_train - roc_auc_test)\n",
    "    \n",
    "    # Perbaikan: Pilih model dengan ROC-AUC Test tertinggi, atau jika sama, dengan score_diff terkecil\n",
    "    if roc_auc_test > best_roc_auc_test or (roc_auc_test == best_roc_auc_test and score_diff < best_score_diff):\n",
    "        best_model = pipe\n",
    "        best_model_name = name\n",
    "        best_roc_auc_test = roc_auc_test\n",
    "        best_score_diff = score_diff\n",
    "\n",
    "print(f\"🏆 Model Terbaik: {best_model_name} dengan ROC-AUC Test tertinggi: {best_roc_auc_test:.3f} dan perbedaan ROC-AUC: {best_score_diff:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Voting Classifier dengan model terbaik\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', forest_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lbgm_model),\n",
    "        ('catb', catb_model)\n",
    "    ],\n",
    "    voting = 'soft'  # Menggunakan probabilitas\n",
    ")\n",
    "\n",
    "# Training Voting Classifier\n",
    "voting_clf.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "# Evaluasi pada Test Set\n",
    "y_test_pred_proba = voting_clf.predict_proba(X_test_mod)[:, 1]\n",
    "\n",
    "# Optimasi threshold menggunakan Precision-Recall Curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_mod, y_test_pred_proba)\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "valid_idx = (precisions >= 0.5) & (recalls >= 0.5)\n",
    "best_threshold = thresholds[valid_idx][0] if len(thresholds[valid_idx]) > 0 else 0.5\n",
    "\n",
    "# Prediksi menggunakan threshold optimal\n",
    "y_test_pred_custom = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluasi Voting Classifier\n",
    "print(\"\\n=== Classification Report (Voting Classifier) ===\")\n",
    "print(classification_report(y_test_mod, y_test_pred_custom))\n",
    "\n",
    "roc_auc_voting = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "print(f\"ROC-AUC Score (Voting Classifier): {roc_auc_voting:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menentukan model terbaik antara pipeline terbaik dan voting classifier\n",
    "final_model = None\n",
    "final_model_name = \"\"\n",
    "\n",
    "if roc_auc_voting > best_roc_auc_test:\n",
    "    final_model = voting_clf\n",
    "    final_model_name = \"voting_clf\"\n",
    "    final_roc_auc = roc_auc_voting\n",
    "\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_model_name = best_model_name\n",
    "    final_roc_auc = best_roc_auc_test\n",
    "\n",
    "print(f\"\\n✅ Model Terbaik untuk Cross-Validation: {final_model_name} dengan ROC-AUC: {final_roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Optimal CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar nilai CV yang ingin diuji\n",
    "cv_values = [3, 5, 7, 10]\n",
    "testing_best_cv = None\n",
    "testing_cv_score = 0\n",
    "\n",
    "print(\"🔍 Mencari Nilai CV Optimal dengan StratifiedKFold...\")\n",
    "\n",
    "for cv in cv_values:\n",
    "    print(f\"\\nEvaluasi dengan cv = {cv}\")\n",
    "    \n",
    "    # Gunakan StratifiedKFold\n",
    "    stratified_cv = StratifiedKFold(n_splits = cv, shuffle = True, random_state = 42)\n",
    "    \n",
    "    # Hitung ROC-AUC menggunakan cross-validation\n",
    "    scores = cross_val_score(final_model, \n",
    "                             X_train_mod, \n",
    "                             y_train_mod, \n",
    "                             cv = stratified_cv,  # ✅ Menggunakan StratifiedKFold sejak awal\n",
    "                             scoring = 'roc_auc', \n",
    "                             n_jobs = -1)\n",
    "    \n",
    "    mean_score = np.mean(scores)  # ✅ Gunakan rata-rata, bukan max\n",
    "    print(f\"ROC-AUC rata-rata: {mean_score:.3f} (dengan cv = {cv})\")\n",
    "    \n",
    "    # Simpan nilai CV terbaik\n",
    "    if mean_score > testing_cv_score:\n",
    "        testing_cv_score = mean_score\n",
    "        testing_best_cv = cv\n",
    "\n",
    "print(f\"\\n✅ Nilai CV Optimal: {testing_best_cv} dengan ROC-AUC: {testing_cv_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Gunakan CV terbaik untuk hyperparameter tuning\n",
    "cv_strat = StratifiedKFold(n_splits = testing_best_cv, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔄 Evaluasi Ulang Model dengan CV Optimal\n",
    "# print(\"\\n🔄 Evaluasi Model Terbaik dengan CV Optimal...\")\n",
    "\n",
    "# # Gunakan CV terbaik dengan StratifiedKFold\n",
    "# cv_strat = StratifiedKFold(n_splits = testing_best_cv, shuffle = True, random_state = 42)\n",
    "\n",
    "# # Evaluasi ulang dengan CV terbaik\n",
    "# strat_cv_scores = cross_val_score(final_model, \n",
    "#                                   X_train_mod, \n",
    "#                                   y_train_mod, \n",
    "#                                   cv = cv_strat, \n",
    "#                                   scoring = 'roc_auc', \n",
    "#                                   n_jobs = -1)\n",
    "\n",
    "# mean_strat_roc_auc = np.mean(strat_cv_scores)  # ✅ Gunakan mean, bukan max\n",
    "# print(f\"🏆 Final ROC-AUC Score dengan CV Optimal: {mean_strat_roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "halving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih model yang akan di-tuning\n",
    "if final_model_name == \"Random Forest\":\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 300, 500, 1000],  # Jumlah pohon dalam hutan\n",
    "        'max_depth': [10, 20, 30, None],  # Kedalaman maksimum pohon\n",
    "        'min_samples_split': [2, 5, 10, 20],  # Minimum sampel untuk melakukan split\n",
    "        'min_samples_leaf': [1, 2, 5, 10],  # Minimum sampel di setiap daun\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],  # Fitur yang dipertimbangkan per split\n",
    "        'bootstrap': [True, False]  # Apakah menggunakan bootstrap sampling\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"XGBoost\":\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],  # Jumlah pohon boosting\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2],  # Laju pembelajaran\n",
    "        'max_depth': [3, 6, 10, 15],  # Kedalaman maksimum pohon\n",
    "        'min_child_weight': [1, 3, 5, 7],  # Bobot minimum anak untuk split\n",
    "        'subsample': [0.6, 0.8, 1.0],  # Rasio sampel yang digunakan dalam training\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],  # Proporsi fitur yang digunakan per pohon\n",
    "        'gamma': [0, 0.1, 0.2, 0.5],  # Pengendalian pruning dengan minimum loss reduction\n",
    "        'reg_lambda': [0, 0.1, 1, 10],  # Regularisasi L2\n",
    "        'reg_alpha': [0, 0.1, 1, 10]  # Regularisasi L1\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"LightGBM\":\n",
    "    param_dist = {\n",
    "        'num_leaves': [31, 50, 100, 150],  # Jumlah daun pada setiap pohon\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2],  # Laju pembelajaran\n",
    "        'n_estimators': [100, 200, 300, 500],  # Jumlah pohon boosting\n",
    "        'max_depth': [-1, 10, 20, 30],  # Kedalaman maksimum pohon (-1 berarti tidak terbatas)\n",
    "        'min_child_samples': [10, 20, 50, 100],  # Minimum sampel dalam satu leaf\n",
    "        'subsample': [0.6, 0.8, 1.0],  # Proporsi sampel yang digunakan\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],  # Proporsi fitur yang digunakan per pohon\n",
    "        'reg_lambda': [0, 0.1, 1, 10],  # Regularisasi L2\n",
    "        'reg_alpha': [0, 0.1, 1, 10]  # Regularisasi L1\n",
    "    }\n",
    "\n",
    "elif final_model_name == \"CatBoost\":\n",
    "    param_dist = {\n",
    "        'iterations': [100, 200, 300, 500],  # Jumlah iterasi boosting\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2],  # Laju pembelajaran\n",
    "        'depth': [4, 6, 10, 12],  # Kedalaman maksimum pohon\n",
    "        'l2_leaf_reg': [1, 3, 5, 10],  # Regularisasi L2 untuk leaf\n",
    "        'bagging_temperature': [0.1, 0.5, 1, 2],  # Kontrol bootstraping (mirip dengan subsample)\n",
    "        'border_count': [32, 64, 128],  # Jumlah bin untuk fitur numerik\n",
    "        'random_strength': [0.1, 0.5, 1, 2]  # Noise untuk regularisasi\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Bersihkan dataset dari NaN/Inf\n",
    "if np.any(pd.isnull(X_train_mod)) or np.any(np.isinf(X_train_mod)):\n",
    "    print(\"⚠️ Warning: Dataset mengandung NaN atau Inf!\")\n",
    "    X_train_mod = X_train_mod.fillna(X_train_mod.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Hitung jumlah kombinasi parameter\n",
    "total_combinations = len(list(product(*param_dist.values())))\n",
    "print(f\"Total kombinasi parameter valid: {total_combinations}\")\n",
    "\n",
    "# 🔹 Atur `n_candidates` agar tidak lebih besar dari jumlah kombinasi parameter\n",
    "n_candidates = min(100, total_combinations)  # Ambil nilai yang masuk akal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cv_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in cv_strat.split(X_train_mod, y_train_mod):\n",
    "    y_train_fold, y_test_fold = y_train_mod.iloc[train_idx], y_train_mod.iloc[test_idx]\n",
    "    print(f\"Train class distribution: {Counter(y_train_fold)}\")\n",
    "    print(f\"Test class distribution: {Counter(y_test_fold)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# 🔍 Hyperparameter Tuning dengan HalvingRandomSearchCV\n",
    "halving_search = HalvingRandomSearchCV(\n",
    "    final_model, \n",
    "    param_distributions = param_dist, \n",
    "    factor = 2, \n",
    "    scoring = 'roc_auc', \n",
    "    cv = 3, \n",
    "    n_jobs = -1, \n",
    "    random_state = 42,\n",
    "    n_candidates = n_candidates,  # Pastikan tidak lebih dari kombinasi parameter\n",
    "    error_score = \"raise\"\n",
    ")\n",
    "\n",
    "halving_search.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "print(f\"\\n🔍 Hyperparameter Terbaik ({final_model_name}): {halving_search.best_params_}\")\n",
    "print(f\"✅ Best ROC-AUC Score: {halving_search.best_score_:.3f}\")\n",
    "\n",
    "optimal_cv = halving_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔍 Hyperparameter Tuning dengan HalvingRandomSearchCV\n",
    "# halving_search = HalvingRandomSearchCV(\n",
    "#     final_model, \n",
    "#     param_distributions = param_dist, \n",
    "#     factor = 2, \n",
    "#     scoring = 'average_precision', \n",
    "#     cv = cv_strat, \n",
    "#     n_jobs = -1, \n",
    "#     random_state = 42,\n",
    "#     n_candidates = n_candidates,  # Pastikan tidak lebih dari kombinasi parameter\n",
    "#     error_score = \"raise\"\n",
    "# )\n",
    "\n",
    "# halving_search.fit(X_train_mod, y_train_mod)\n",
    "\n",
    "# print(f\"\\n🔍 Hyperparameter Terbaik ({final_model_name}): {halving_search.best_params_}\")\n",
    "# print(f\"✅ Best ROC-AUC Score: {halving_search.best_score_:.3f}\")\n",
    "\n",
    "# optimal_cv = halving_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔹 Kalibrasi Model untuk Probabilitas yang Lebih Akurat\n",
    "# calibrated_model = CalibratedClassifierCV(optimal_cv, method='sigmoid', cv=5)\n",
    "# calibrated_model.fit(X_train_mod, y_train_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Prediksi dengan Model Terbaik\n",
    "y_test_pred_proba = optimal_cv.predict_proba(X_test_mod)[:, 1]\n",
    "\n",
    "# 🔹 Mencari Threshold Optimal\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_mod, y_test_pred_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "best_threshold = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# 🔹 Gunakan Threshold Optimal untuk Prediksi Akhir\n",
    "y_test_pred = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# 🔹 Classification Report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test_mod, y_test_pred))\n",
    "\n",
    "# 🔹 ROC-AUC Score\n",
    "roc_auc_final = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "print(f\"🎯 Final ROC-AUC Score: {roc_auc_final:.3f}\")\n",
    "print(f\"🔹 Best Threshold Used: {best_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔹 Prediksi dengan Model Terbaik\n",
    "# y_test_pred_proba = calibrated_model.predict_proba(X_test_mod)[:, 1]\n",
    "\n",
    "# # 🔹 Mencari Threshold Optimal\n",
    "# precisions, recalls, thresholds = precision_recall_curve(y_test_mod, y_test_pred_proba)\n",
    "# f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "# best_threshold_f1 = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# # 🔹 Alternatif Threshold Berdasarkan Youden’s J Index\n",
    "# fpr, tpr, roc_thresholds = roc_curve(y_test_mod, y_test_pred_proba)\n",
    "# youden_index = tpr - fpr\n",
    "# best_threshold_youden = roc_thresholds[youden_index.argmax()]\n",
    "\n",
    "# # 🔹 Gunakan Threshold Optimal untuk Prediksi Akhir\n",
    "# final_threshold = (best_threshold_f1 + best_threshold_youden) / 2  # Ambil rata-rata dari keduanya\n",
    "# y_test_pred = (y_test_pred_proba >= final_threshold).astype(int)\n",
    "\n",
    "# # 🔹 Classification Report\n",
    "# print(\"\\n=== Classification Report ===\")\n",
    "# print(classification_report(y_test_mod, y_test_pred))\n",
    "\n",
    "# # 🔹 ROC-AUC Score\n",
    "# roc_auc_final = roc_auc_score(y_test_mod, y_test_pred_proba)\n",
    "# print(f\"🎯 Final ROC-AUC Score: {roc_auc_final:.3f}\")\n",
    "# print(f\"🔹 Best Threshold Used: {final_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 1. Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_mod, y_test_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Deteksi Fraud')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 2. Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recalls, precisions, color='red', lw=2, label='Precision-Recall Curve')\n",
    "plt.axvline(recalls[np.argmax(f1_scores)], color='black', linestyle=\"--\", label=\"Best F1-Score Threshold\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Deteksi Fraud')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 3. Confusion Matrix\n",
    "cm = confusion_matrix(y_test_mod, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Deteksi Fraud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 4. Distribusi Probabilitas Prediksi\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_test_pred_proba[y_test_mod == 0], bins=50, color='blue', label='Non-Fraud', kde=True)\n",
    "sns.histplot(y_test_pred_proba[y_test_mod == 1], bins=50, color='red', label='Fraud', kde=True)\n",
    "plt.axvline(best_threshold, color='black', linestyle=\"--\", label=\"Best Threshold\")\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribusi Probabilitas Prediksi')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
